---
title: "Communities and Crime"
subtitle: "Statistical Learning Final Exam Project"
chapter: "Other models and Conclusions"
author: "Caria Natascia, Cozzolino Claudia, Petrella Alfredo"
date: "June 20, 2020"
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
#set seed
set.seed(1234)

# import required structures

#dataframes
cleandf <- read.csv("crimedata-cleaned.csv", row.names = 1)

standardization <- function(x) {
return ((x - mean(x)) / sd(x))
}

standf <- cleandf
standf[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], standardization)
coltodrop <- c(1,2, seq(103,120)[-17]) # -17 keeps ViolentCrimesPerPop

df <- standf[,-coltodrop]

rem9 <- c( 
"population","agePct16t24","numbUrban","pctWSocSec","medFamInc","perCapInc",
"NumUnderPov","PctLess9thGrade","PctOccupMgmtProf","MalePctDivorce","FemalePctDiv","PctFam2Par",
"PctKids2Par" ,"NumKidsBornNeverMar", "PctImmigRec5","PctImmigRec10",      
"PctRecImmig5","PctRecImmig8","PctRecImmig10","PctSpeakEnglOnly" ,  
"PctLargHouseOccup","PersPerOccupHous","PctHousOwnOcc","OwnOccLowQuart",     
"OwnOccHiQuart","RentLowQ","RentHighQ","MedRent",            
"NumInShelters","NumStreet","PctForeignBorn")

rem8 <- c(rem9,
"householdsize","racePctWhite","agePct12t29","medIncome",
"pctWWage","pctWPubAsst","PctPopUnderPov","PersPerFam",         
"PctYoungKids2Par","PctWorkMom","PctKidsBornNeverMar", "PctImmigRec8",       
"PctNotSpeakEnglWell", "PctPersDenseHous","OwnOccMedVal","RentMedian",         
"PctSameCity85")

corrdf <- df[,!(colnames(df) %in% rem8)]

#colors
Col = c("#EF476F", "#FFD166", "#06D6A0", "#118AB2", "#073B4C")
Rgb = c(rgb(0.94, 0.28, 0.44, 0.7),
        rgb(1, 0.82, 0.4, 0.7),
        rgb(0.02, 0.84, 0.63, 0.7),
        rgb(0.07, 0.54, 0.7, 0.7),
        rgb(0.03, 0.23, 0.3, 0.7))

# Train-Test samples
train.sample <- as.numeric(sample(rownames(corrdf), 0.8*dim(corrdf)[1]))
test.sample <- as.numeric(setdiff(rownames(corrdf), train.sample))

```


# Other possible predictive models

In this section other models have been tested for the prediction of the violent and non violent rate var variables.

## Add categorical predictor

Until now we assumed that the response variable ViolentCrimesPerPop has equal behaviour between the different states. Even if in practice this assumption considerably simplified the model, the hypothesis of similar distribution between different cities belonging to a vast territory such as the American one, characterized by a wide variety of geographic, cultural and social conditions, could be a too strong request.
Below a side by side boxplot of ViolentCrimesPerPop between the different states is presented (using the standardized and log-transformed version of the data). The horizontal line draws the mean value between all data without distinction.

```{r, fig.height=10, fig.width=16}
boxplot(log(standf$ViolentCrimesPerPop+1)~standf$state, xlab="state",
        ylab=expression(log(ViolentCrimesPerPop+1)), col= Col)
abline(h=mean(log(standf$ViolentCrimesPerPop+1)), col = "red")
```

The plot seems to confirm the doubts discussed above, therefore we tested the linear model considering the categorical variable State as factor.

```{r, warning=FALSE}
XS = subset(corrdf, select = - c(ViolentCrimesPerPop))
XS["state"]=standf$state

is.factor(XS$state) 

XS.train <- subset(XS, (row.names(XS) %in% train.sample))
XS.test <- subset(XS, (row.names(XS) %in% test.sample))

Y <- log(corrdf["ViolentCrimesPerPop",drop=FALSE] +1)

Y.train <- unlist(subset(Y, (row.names(Y) %in% train.sample)))
Y.test <- unlist(subset(Y, (row.names(Y) %in% test.sample)))

regc.out <- lm(Y.train~. , data=XS.train) #application of LR with a categorical variable
summary(regc.out)
```

From the results plotted by the summary, it seems that adding this information does not help too much the model, the State variable seem not very influential. The only exceptions are the Maine and North Dakota states (ME and ND) for which the information is more significant. Analyzing the position of these two territories it is interesting to note that they are both two northern states on the border with Canada, this precise geographical diversity could be the reason for our result.

Testing the model, as expected the MSE is slightly better than the equivalent model withot the categorical information, otherwise, the adjusted $R^2$ is smaller.


```{r}
# prediction 
pred = predict(regc.out, XS.test)

n <- dim(XS.test)[1]
p <- 94

rss <- sum((Y.test - pred)^2)       # Residual Sum of Squares
ess <- sum((pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

Given the contradictory results between the boxplot and the model, we decided to conduct a deeper study by performing ANOVA, Tukey's test and ANCOVA on our dataset.
As expected, the ANOVA gives a very small p-value, confirming the theory of not all equal means. At this point hence the Tukey method for all pairwise mean comparisons is applied.

```{r}
summary(aov.state <- aov(ViolentCrimesPerPop ~ state, data = standf))
#reject all equal mean hyphothesis?

result<-TukeyHSD(aov.state, "state", ordered = TRUE)

result<-as.data.frame(result$state)

head(result[result[,4]<0.05,],10) #where reject equal mean hyphothesis?

```

Examining all the pairs in which Tukey's test allows to reject the hypothesis, i.e. where the adjusted p-value is less than $\alpha=0.05$, it can be concluded that the states can be grouped in two main cluster. Also in this case it is interesting to observe how the statistical test result can be interpreted geographically, the two clusers exactly divide the USin two regions, making us hypothesize that the variable of interest may have influences related to social and cultural aspects related to the position.  

```{r, warning=FALSE}
library(ggplot2)
library(usmap)

cluster1 <- c("ND","ME","WI","NH","SD","UT","CT","ID","WY","PA","RI","OR","OH","IA",
              "NJ","WV","MA","MO","OK","IN","VA","CO","WA","AZ","NV","AK","NY","TX",
              "MS","KY","AR","TN","CA","NM","DE")
cluster2 <- c("NC","GA","AL","FL","MD","SC","LA","DC")

XS["Cluster"]=rep(1,1996)
XS$Cluster[XS$state %in% cluster2] <- 2

plot_usmap(labels = T,
    data = XS[c("state","Cluster")], values = "Cluster",  color = "black") + 
    scale_fill_gradient(low = Col[2], high =  Col[1]) + 
  theme(legend.position = "none")+
    labs(title = "2 clusters by ViolentCrimesPerPop Mean similarity")
```
The linear model considering a new categorical predictor, that factorize the cluster to which each state belongs, is then built. The cluster information is now significative, moreover the results on the test set show a decrease of the MSE and equal adjusted $R^2$ (compared to the equivalent model without categorical predictor).

```{r}
XS = XS[,-53]
XS$Cluster <- as.factor(XS$Cluster)

is.factor(XS$Cluster) 

XS.train <- subset(XS, (row.names(XS) %in% train.sample))
XS.test <- subset(XS, (row.names(XS) %in% test.sample))


regc.out <- lm(Y.train~. , data=XS.train) #application of LR with a categorical variable
summary(regc.out)
```


```{r}
# prediction 
pred = predict(regc.out, XS.test)

n <- 400
p <- 53

rss <- sum((Y.test - pred)^2)       # Residual Sum of Squares
ess <- sum((pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

Finally ANCOVA is performed, to asses if the two models differ between the two groups. The previous summary proves that the intercepts are different, now we want to test if also some of the slope coefficients vary between the two clusters.
To this end the linear model exploiting also interaction effects between the Cluster variable and all the other is built.

```{r}
#add interaction and main effect due to Cluster
regc.out <- lm(Y.train~.*Cluster , data=XS.train) 
summary(regc.out)
```

```{r}
# prediction 
pred = predict(regc.out, XS.test)

n <- dim(XS.test)[1]
p <- 105

rss <- sum((Y.test - pred)^2)       # Residual Sum of Squares
ess <- sum((pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

The summary shows that there are significative slope changes for coefficients like agePct12t21, blackPerCap, pctWFarmSelf, PctEmploy, MalePctNevMarr and MedRentPctHousInc. Note that many of these are right among the more influential predictors emerged from the feature selection section.
However this more complex model does not perform better than others in terms of MSE, as a matter of fact the adjusted $R^2$ is considerably smaller.

In conclusion, the contribution of the variable State, although it seems to have influences on the response, it does not seem to bring many improvements to the simplified model tested so far. Moreover not having considered it does not therefore seem to be problematic, making us believe that the model adopted is a good compromise between valid representation of data and simplicity.

## Multiple Responses Linear Regression

We conclude the experimentation of continuous regression models testing the contemporary prediction of all the crime rates related response variables: murdPerPop, rapesPerPop, robbbPerPop, assaultPerPop, burglPerPop, larcPerPop, autoTheftPerPop, arsonsPerPop, ViolentCrimesPerPop and nonViolPerPop (with reasonings similar to those made above, logarithmic transformation was used for each of them).
Note that this variables are correlated between them, and hence it is reasonably  to expect that they depend on the same predictors, this is the main motivation to build such model, moreover training the regressor considering all of them could have benefit effect. 
As a matter of fact, the MSE results on the test are good for almost all the responses.

```{r}
#predict all the crime features per pop 
X = subset(corrdf, select = - c(ViolentCrimesPerPop))
X.train <- subset(X, (row.names(X) %in% train.sample))
X.test <- subset(X, (row.names(X) %in% test.sample))


Ys<-cbind(log(standf["murdPerPop"]+1), log(standf["rapesPerPop"]+2), 
          log(standf["robbbPerPop"]+1), log(standf["assaultPerPop"]+1),
          log(standf["burglPerPop"]+2), log(standf["larcPerPop"]+2), 
          log(standf["autoTheftPerPop"]+1), log(standf["arsonsPerPop"]+1),
          log(standf["ViolentCrimesPerPop"]+1), log(standf["nonViolPerPop"]+2))

Ys.train<- as.matrix(subset(Ys, (row.names(Ys) %in% train.sample)))
Ys.test<- as.matrix(subset(Ys, (row.names(Ys) %in% test.sample)))

mm <- lm( Ys.train ~., data=X.train)
#summary(mm) #long output
```


```{r}
# predictions 
n <- 400
p <- 52

pred = predict(mm, X.test)
rss <- colSums((as.data.frame(Ys.test) - as.data.frame(pred))^2)# Residual Sum of Squares
ess <- colSums((pred - mean(as.matrix(Ys.test)))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n

as.data.frame(cbind(mse,adjr2))
```



## Binary classification models

The last section of our research is dedicated to binary classification. The capability of logistic regressor and KNN algorithm have been tested in the classification task among two level of ViolentCrimesPerPop: High and Low. 
In particular, we set a threshold in order to distinguish if a community has high level of crimes per population. Different options were possible for the value of this threshold such as considering the mean among all the US provided in [6](http://www.disastercenter.com/crime/uscrime.htm), the mean or the median of our entire dataset.  

For convenience in this simplistic study, we decided to proceed using the median of ViolentCrimesPerPop, the main reasons of this choice are that, first of all, it is more representative than the mean since the distribution is skewed, secondly this value split the data in balanced classes.

```{r}

#HLthreshold <- 0 # threshold is the mean (0 since standardized)

# threshold is the median (balanced classes)
HLthreshold <- median(standf$ViolentCrimesPerPop) 

YF.train <- Y.train
YF.train[Y.train > HLthreshold] <- "High"
YF.train[!Y.train > HLthreshold] <- "Low"
YF.train <- as.factor(YF.train)

YF.test <- Y.test
YF.test[Y.test > HLthreshold] <- "High"
YF.test[!Y.test > HLthreshold]<- "Low"
YF.test <- as.factor(YF.test)

```

### Logistic regression

The logistic regression is built below.
```{r}
lreg.out<-glm(YF.train~., family = binomial, data=X.train)

# check the coding of ViolentCrimesFactor
contrasts(YF.train)

logistic.prob <- predict(lreg.out, type="response") # want probability
```

In order to select the best threshold for the probability, the ROC curve is plotted. The shape of the function and the AUC value confirm the acceptable quality of the model.
```{r, fig.height=5, fig.width=5, warning=FALSE}
# ROC curve 
library(pROC)
# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(YF.train, logistic.prob, levels=c("Low", "High"), transpose = TRUE)

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate",
     ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
rxy <- coords(roc.out, "best")
rxy
```

```{r}
logistic.pred <- rep("High", 1596)
logistic.pred[logistic.prob>rxy[,1]] <- "Low"

#train accuracy
table(logistic.pred,YF.train)
mean(logistic.pred==YF.train)

```

```{r}
#test accuracy
logistic.prob <- predict(lreg.out, X.test, type="response") # want probability
logistic.pred <- rep("High", 400)
logistic.pred[logistic.prob>rxy[,1]] <- "Low"

table(logistic.pred,YF.test)
mean(logistic.pred==YF.test)
```

With this choice of the probability threshold, the model achieves good sensitivity and specificity and an accuracy on the test set of 85%.

### KNN[V]

Finally we improved a completely non-parametric and non-linear approach: KNN. In this method no assumptions are made about the shape of the decision boundary, moreover differently from LDA or QDA, normality condition of the predictors are not necessary.

Strictly speaking, we first performed 10 folds cross validation to choose the best value for k, i.e. the number of closest training observations to consider to assign the majority class to a new point.

```{r, warning=FALSE}
#10 fold cross validation
XY.train=as.data.frame(cbind(X.train,YF.train))
XY.train$YF.train<- as.factor(XY.train$YF.train)
is.factor(XY.train$YF.train)

# install.packages("caret", dependencies = T)
library(caret)

trControl <- trainControl(method  = "cv",number  = 10)

fit <- train(YF.train ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = XY.train)

fit
```

From the C.V., the best k is 5, then the model is performed with this value on the test set, returning an accuracy of 81.5%.
Note that the performance is slightly poorer than the linear, confirming us the power of this simple and fully interpretable model even with a large dataset, with many possible correlated features, as the one under investigation.

```{r, warning=FALSE}
library(class)
# test accuracy
knn.pred <- knn(X.train, X.test, YF.train, k=5)
table(knn.pred,YF.test)
mean(knn.pred==YF.test)
```


Let's have a brief recap and see what we have learned.
We started with an overall exploration of the dataset, handling, in the meanwhile,
null and corrupt data, and choosing as interesting to predict among the others
the proportion of violent crimes per 100K population.
At this point we investigated the reletions between our target variable
and the rest of the dataset, achieving the first meaningful result: the extraction,
via EDA before and different statistical tools after,
of a few topics which mainly determine the output of the models.
In particular, the most significant clusters are identified, in descending influence order,
by the family structure, the marital status, the economic status and the ethnicity
of the population of each city.
We then applied a multiple linear regression, and increasingly improved it checking for outliers,
high leverage points, collinearity issues, and testing all kinds of variable selection methods
comparing them and eventually cumulating the obtained results.
We furthermore evaluated the influence of the categorical variable "state" on the prediction
with ANOVA, Tukey's test and ANCOVA and we built a multiple responses L.R. to check how it would perform.
At the end, for the sake of curiosity, we built two binary classification models, giving a threshold
to split "ViolentCrimesPerPop" in the "High" and "Low" classes: a classical logistic regrassion and a KNN
to test a non-linear and non-parametric approach.

As for the effectiveness of the model, it turned out that the binary classifiers have a rewarding accuracy of 80-85%,
while the regression models reach barely sufficient results: in the Principal Component regression, even with a valuable
adjusted $R^2$ of 65%, the logarithmic tranformation applied to the target variable led us to compute a MSLE of 0.35,
which says that the average ratio between the predicted value +1 and the true value +1 is $\sqrt{e^\text{MSLE}}=1.81^{(\pm 1)}$. This is mainly due to the fact that a linear model (and in general a polinomial one, which gave similar results), is not enough to fit the complexity of the data, as we expected given the complexity of the task even from a social perspective. Furthermore, a huge limitation was the really poor quality of the LEMAS dataset, the one gathered from the police departments, and this didn't allow us to perform a parallel study to investigate a more local point of view.

At the end of the story, we tried to apply our knowledge in statistical learning to reach a satisfying level of comprehension
of our data and the structures behind them, realizing that this is a task that goes beyond the concatenation of mathematical models,
involving creativity, newness and dedication. The Communities and Crime data set has been a challenge for us, and only now that we know more
about the data and the questions they may hide, we can eventually afford to apply deep learning models to see if the results can be improved.


\newpage
# Technical appendix

## [I] Mean Squared Logarithmic Error

The logarithmic transformation $\tilde y = \log(y+1)$ applied to the target variable makes the MSE take the form $\frac{\sum{(\log(y_T+1)-\log(y_P+1))^2}}{n}=\frac{\sum{(\log(\frac{y_T+1}{y_P+1}))^2}}{n}$, resulting in a measure that
only cares about the percentual difference between the true and the predicted values. Moreover, in our case we don't want large errors to be significantly more penalized than small ones, due to the presence of cities with significantly high values with respect to the mean making the range of the target value large.
To read more about the Mean Squared Logarithmic Error, see [9].

## [II] Ridge regression

Ridge regression is pretty similar to the MLR, except that the coefficients
are not estimated by minimizing the usual $RSS=\sum_{i=1}^n (y_i-\beta_0-\sum_{j=1}^p\beta_j x_{ij})^2$
but the modified expression $RSS+\lambda\sum_{j=1}^p\beta_j^2$ where $\lambda\geq 0$ can be tuned in order
to *shrink* the estimates of the $\beta_i$ (but not the intercept!) to 0.
To read more about Ridge regression, see [1], 215.

## [III] Lasso

The lasso is an alternative to the Ridge regression that takes into account its main disadvantage:
Ridge regression, in fact, shrinks all the coefficients towards 0 never pulling them down to actually 0.
The only difference for the Lasso is the choice of an $L^1$ normalization function, which is known by theory
to be a **sparse metric**, allowing the method to perform an automatic variable selection.
To read more about LASSO regression, see [1], 219.

## [IV] Principal Component Analysis

PCA is a technique for reducing the dimension of a n$\times$p data matrix X.
In our case, we apply it to our dataframe matrix, in order to perform, somehow, a variable selection.
Somehow in the sense that the variables are not just selected, but linearly combined in new ones which
are chosen orthogonal and sorted by the variance of the model they can explain, assuming it as a synonym of information.
The first principal component direction, in particular, is that along which the observations vary the most,
and so are the next ones among the left orthogonal directions left.
To read more about the linear algebra involved in the PCA, see [10].

## [V] K-Nearest Neighbors classifier

A KNN is a non-parametric and non-linear method which offers a good alternative when linear models are not suitable.
In KNN classification, the output is a class membership. An object is assigned to the most common class among its K nearest neighbors, where K is a positive integer. Note that the results is strongly local, because it just depends on the nearest K training instances to the target point.
To read more about KNN, see [1], 39.




\newpage

# References

[1] "An Introduction to Statistical Learning", G. James, D. Witten, T. Hastie and R. Tibshirani, Springer, 2013.

[2] "DEA History Book, 1876-1990" (drug usage & enforcement), US Department of Justice, 1991, USDoJ.gov, webpage: DoJ-DEA-History-1985-1990.

[3] "Guns and Violence: The Enduring Impact of Crack Cocaine Markets on Young Black Males", W.N. Evans, G. Garthwaite, T. Moore, 2018.

[4] "Measuring Crack Cocaine and Its Impact", Fryer, Roland. Harvard University Society of Fellows: 3, 66. Retrieved January 4, 2016.

[5] "The New Jim Crow: Mass Incarceration in the Age of Colorblindness", M. Alexander.

[6] http://www.disastercenter.com/crime/uscrime.htm

[7] https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized

[8] https://online.stat.psu.edu/stat462/node/171/.

[9] https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)

[10] https://towardsdatascience.com/the-mathematics-behind-principal-component-analysis-fff2d7f4b643






