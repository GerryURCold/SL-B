---
title: "CodiceAvanzato"
author: "Gitteam"
date: "9/6/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Non mi picchiate, se si usa la media come centro scale Ã¨ equivalente a questa... Alla fine ho imparato a leggere (rdocumentation.org): *the root-mean-square for a (possibly centered) column is defined as $\sqrt\Sigma(X^2)/(n-1)$, where $x$ is a vector of the non-missing values and $n$ is the number of non-missing values. In the case `center = TRUE`, this is the same as the standard deviation, but in general it is not (to scale by the standard deviations without centering, use `scale(x, center = FALSE, scale = apply(x, 2, sd, na.rm = TRUE))`.*

```{r}
# Standardization using an home made function 
standardization <- function(x) {
return ((x - mean(x)) / sd(x))
}

standf <- cleandf
standf[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], standardization)
```

```{r}
# Normalization using an home made function 
normalization <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

normdf <- cleandf
normdf[seq(3,dim(normdf)[2])] <- lapply(cleandf[seq(3,dim(cleandf)[2])], normalization)
```

```{r, eval=FALSE, include=FALSE}
scaleddf <- data.frame(cleandf2)
scaleddf['population'] <- scale(cleandf['population'], center = rep(1/2))
# scale fa questo (con center=F): col/sqrt(sum(col^2)/(length(col)-1))
# leggi tre celle sopra, perdono
```

```{r}
# Normalization of the dataset without drastic outliers
normnewdf <- newdf
normnewdf[seq(3,dim(normnewdf)[2])] <- lapply(newdf[seq(3,dim(newdf)[2])], normalization)


normnewdfcorr <- newdfcorr
normnewdfcorr[seq(3,dim(normnewdfcorr)[2])] <- lapply(newdfcorr[seq(3,dim(newdfcorr)[2])], normalization)
```

```{r}
# Remove all variables from environment excpet ...
rm(list=setdiff(ls(), c("crimedata", "cleandf", "scaledf", "standf", "normdf", "newdf", "normnewdf","cleandfcorr","newdfcorr","normnewdfcorr","df", "train")))
```

```{r, eval=FALSE, include=FALSE}
C <- cov(SVDdf)

# plot the original singular values (our actual diagonal is squared)
#barplot(sqrt(C.sv$d)*(dim(SVDdf)[1]-1))
barplot(sv$d)

sv$d
#sqrt(C.sv$d)*(dim(SVDdf)[1]-1)

#???
```


```{r}
outdf <- cleandf
outdf[seq(3,dim(outdf)[2])] <- lapply(cleandf[seq(3,dim(cleandf)[2])], is.out.sd)

odf <- inspect.na(outdf, hist = FALSE, byrow=TRUE, barplot = FALSE, na.value = TRUE)
rowtodrop <- odf$row_name

newdf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]


outdf <- cleandfcorr
outdf[seq(3,dim(outdf)[2])] <- lapply(cleandfcorr[seq(3,dim(cleandfcorr)[2])], is.out.sd)

odf <- inspect.na(outdf, hist = FALSE, byrow=TRUE, barplot = FALSE, na.value = TRUE)
rowtodrop <- odf$row_name

newdfcorr <- cleandfcorr[!(rownames(cleandfcorr) %in% rowtodrop),]
```

```{r, eval=FALSE, include=FALSE}
# eventually useful code

# altri comandi per trovare highly correlated... (aiuto non volevo mettermi alla tv :3)
rs <- cor(corrdf, use="complete.obs")
diag(rs) <- NA

# rimuove tutte le colonne che abbiano almeno una correlazione, perdiamo informazioni :/
col_has_over_90 <- apply(rs, 2, function(x) any(x > .9, na.rm = TRUE))

colnames(rs[, col_has_over_90])
# vs
colnames(corrdf)[rem9]

# selezionate a mano e interpretando significato
coltodropcorr <-
c("agePct16t24","numbUrban","medFamInc","perCapInc","PctNotHSGrad","PctBSorMore","MalePctDivorce","FemalePctDiv","NumInShelters",
"MedRent","PctPersOwnOccup","PctHousOwnOcc","PctKids2Par","PctYoungKids2Par","PctImmigRec8","PctRecImmig8","PctImmigRecent",
"PctRecentImmig")
```

# Split in Train and Test / validation Folds
da mettere prima dei primi modelli
```{r}
train <- sample(rownames(cleandf), 1600)
```

```{r}
# train and test MSE

# linear model with training data
lm.fit <- lm(df$ViolentCrimesPerPop~., data=df, subset=train)


# mean squared error on train
y.pred <- predict(lm.fit, newdata=df[train, ])
MSE <- mean((df$ViolentCrimesPerPop[train]-y.pred)^2)
MSE

# mean squared error on test
y.pred <- predict(lm.fit, newdata=df[-train, ])
MSE <- mean((df$ViolentCrimesPerPop[-train]-y.pred)^2)
MSE 
```

```{r}
#train and test MSE

# linear model with training data
lm.fit2 <- lm(log(df$ViolentCrimesPerPop+1)~., data=df, subset=train)


# mean squared error on train
y.pred <- predict(lm.fit2, newdata=df[train, ])
MSE <- mean((log(df$ViolentCrimesPerPop[train]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit2, newdata=df[-train, ])
MSE <- mean((log(df$ViolentCrimesPerPop[-train]+1)-y.pred)^2)
MSE 
```


```{r}
#train and test MSE
to=sample(1:1978,1590)
# linear model with training data
lm.fit3 <- lm(log(outdf$ViolentCrimesPerPop+1)~., data=outdf, subset=to)


# mean squared error on train
y.pred <- predict(lm.fit3, newdata=outdf[to, ])
MSE <- mean((log(outdf$ViolentCrimesPerPop[to]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit3, newdata=outdf[-to, ])
MSE <- mean((log(outdf$ViolentCrimesPerPop[-to]+1)-y.pred)^2)
MSE 
```


```{r}
#train and test MSE

# linear model with training data
lm.fit4 <- lm(log(df$ViolentCrimesPerPop[!whereh]+1)~., data=df[!whereh,], subset=train[-whereh])


# mean squared error on train
y.pred <- predict(lm.fit4, newdata=df[train[-whereh], ])
MSE <- mean((log(df$ViolentCrimesPerPop[train[-whereh]]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit4, newdata=df[-c(train,whereh), ])
MSE <- mean((log(df$ViolentCrimesPerPop[-c(train,whereh)]+1)-y.pred)^2)
MSE 
```

#### Multicollinearity
Unfortunately, not all collinearity problems can be
detected by inspection of the correlation matrix.... VIF? 

```{r}

reg.out2 <- lm(log(df$ViolentCrimesPerPop+1)~., data=df)

sum(vif(reg.out2)>10) #siamo nella cacca? 68 colonne da qui... ignoriamo la cosa e andiamo avanti? piu che atro non ho capito bene come funziona questo vif

```


# Other possible transforms of data
## Standardization

```{r}
# Standardization using scale
scaledf <- cleandf
scaledf[seq(3,dim(scaledf)[2])] <- scale(cleandf[seq(3,dim(cleandf)[2])] )
```

## Outliers
```{r}
# Remove row if one of its value is
# 3 times greater than the upper interquartile bound or
# 3 times lower than the lower interquartile bound

is.out.IQR <- function(x){
  Q <- quantile(x, probs=c(.25, .75))
  iqr <- IQR(x)
  up <-  Q[2]+3*iqr # Upper bound  
  low <- Q[1]-3*iqr # Lower bound
  out <- (x < low | x > up)
  return(out)
}

is.out.sd <- function(x){
  sd <- sd(x)
  up <-  mean(x)+3*sd # Upper bound  
  low <- mean(x)-3*sd # Lower bound
  out <- (x < low | x > up)
  return(out)
}

temp <- standf
temp[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], is.out.IQR)

temp <- inspect.na(temp, hist=FALSE, byrow=TRUE, barplot=FALSE, na.value=TRUE)
rowtodrop_s <- temp$row_name

standf_nO <- standf[!(rownames(standf) %in% rowtodrop_s),]
```

```{r}
# Remove all variables from environment excpet ...
rm(list=setdiff(ls(), c("crimedata", "cleandf", "scaledf", "df", "train")))
```

```{r}
#train and test MSE

# linear model with training data
lm.fit5 <- lm(log(cleandfcorr$ViolentCrimesPerPop+1)~., data=cleandfcorr[-c(1,2, seq(48,65)[-17])], subset=train)


# mean squared error on train
y.pred <- predict(lm.fit5, newdata=cleandfcorr[train,-c(1,2, seq(48,65))])
MSE <- mean((log(cleandfcorr$ViolentCrimesPerPop[train]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit5, newdata=cleandfcorr[-train,-c(1,2, seq(48,65))])
MSE <- mean((log(cleandfcorr$ViolentCrimesPerPop[-train]+1)-y.pred)^2)
MSE 
```

```{r}
#computing H by hand

#X matrix of predictors values
#X<- data.matrix(df[,1:100])
#H <- X %*% solve(t(X) %*% X) %*% t(X)

#diag(H) #uguale ad hatvalues yee
```

no log transf

```{r}
#train and test MSE
tp<-sample(1:1280,1000)
pcdf<-as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])

# linear model with training data
lm.fitPC <- lm(standfcorr$ViolentCrimesPerPop~., data=pcdf, subset=tp)


# mean squared error on train
y.pred <- predict(lm.fitPC, newdata=pcdf[tp,])
MSE <- mean((standfcorr$ViolentCrimesPerPop[tp]-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fitPC, newdata=pcdf[-tp,])
MSE <- mean((standfcorr$ViolentCrimesPerPop[-tp]-y.pred)^2)
MSE 
```

con trasf
```{r}
#train and test MSE
tp<-sample(1:1280,1000)
pcdf<-as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])

# linear model with training data
lm.fitPC <- lm(log(standfcorr$ViolentCrimesPerPop+1)~., data=pcdf, subset=tp)


# mean squared error on train
y.pred <- predict(lm.fitPC, newdata=pcdf[tp,])
MSE <- mean((log(standfcorr$ViolentCrimesPerPop[tp]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fitPC, newdata=pcdf[-tp,])
MSE <- mean((log(standfcorr$ViolentCrimesPerPop[-tp]+1)-y.pred)^2)
MSE 
```
PC regression
```{r}

#use corresponding pc 
pc0 <- lm(standf$ViolentCrimesPerPop~., data=as.data.frame(pc$scores))
summary(pc0)

#use only first 15 comp
pc01 <- lm(standf$ViolentCrimesPerPop~., data=as.data.frame(pc$scores[,1:15]))
summary(pc01)

#use corresponding pc 
pc1 <- lm(standfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores))
summary(pc1)

#use only first 10 comp
pc11 <- lm(standfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores[,1:15]))
summary(pc11)

#use only comp with '***' significativity
pc12 <- lm(standfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)]))
summary(pc12)

plot(pc12)

scatter.smooth(pccorr$scores[,8],standfcorr$ViolentCrimesPerPop)
```
## Singular Value Decomposition

An alternative way to reduce the dimensionality of the dataset is to compute the Singular Value Decomposition on the matrix $X$
associated to the model, obtaining two unitary matrices $U$ and $V$ and a rectangular diagonal matrix of singular values $S$ such that
$X = USV^T$, where the values on the diagonal of $S$ are sorted in descendent order and the first $k$ columns of $V$ select the
correspondent first $k$ most informative combinantions of the attributes of the original dataset, i.e. if we call $V_k$ the matrix
containing the first $k$ columns of $V$, $X_k=XV_k$ is a transformation of our dataset with just $k$ columns that contains the most part
of information of $X$.

```{r}
# scaledf SVD
SVDdf <- scaledf[,3:102]

# SVD on the correlation matrix
sv <- svd(SVDdf)

# plot the singolar values
barplot(sv$d)

# SVD reduced model
k <- 20
V <- sv$v
matrix(SVDdf)

SVDreddf <- data.matrix(SVDdf) %*% V[,1:k]

#C.sv??
# check that U and V are the same in this case
#sum(!C.sv$u-C.sv$v<0.000001)
```

```{r, eval=FALSE, include=FALSE}
C <- cov(SVDdf)

# plot the original singular values (our actual diagonal is squared)
#barplot(sqrt(C.sv$d)*(dim(SVDdf)[1]-1))
barplot(sv$d)

sv$d
#sqrt(C.sv$d)*(dim(SVDdf)[1]-1)

#???
```
## Polynomial Regression

non ci serve, linearity assump soddisfatta
DA TOGLIERE, GIA FATTO SU
```{r, eval=FALSE, include=FALSE}
# fit using a second order polynomial in all var

polyformula <- eval(paste("standfcorr$ViolentCrimesPerPop ~",
paste(paste('poly(', names(as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])), ', 2)', sep=''), collapse=" + ")))

m2 <- lm(polyformula, data=as.data.frame(pccorr$scores))
summary(m2)

plot(m2)



# fit using a 3 order polynomial in all var

polyformula <- eval(paste("standfcorr$ViolentCrimesPerPop ~",
paste(paste('poly(', names(as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])), ', 3)', sep=''), collapse=" + ")))

m3 <- lm(polyformula, data=as.data.frame(pccorr$scores))
summary(m3)

plot(m3)



#usare solo se si hanno poche var in considerazione! dai risultati sembra che gli ordini superiori al primo non siano molto influenti...
```
### LDA e QDA
servono delle assuzioni maggiori sui dati.. da controllare, discuterne
```{r}
lda.fit <- lda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47])
lda.fit

lda.pred <- predict(lda.fit, cleandfcorr[,3:47])
lda.class <- lda.pred$class
table(lda.class,cleandfcorr$ViolentCrimesFactor)

#train acc
mean(lda.class==cleandfcorr$ViolentCrimesFactor)

#train err
(87+183)/1996

qda.fit <- qda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47])
qda.fit
qda.class <- predict(qda.fit,cleandfcorr[,3:47])$class
table(qda.class,cleandfcorr$ViolentCrimesFactor)
#train err
(118+189)/1996
#train acc
mean(qda.class==cleandfcorr$ViolentCrimesFactor)
```


```{r}
#using train and test
lda.fit <- lda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47], subset=train)
#lda.fit

lda.pred <- predict(lda.fit, cleandfcorr[,3:47])
lda.class <- lda.pred$class
table(lda.class,cleandfcorr$ViolentCrimesFactor)

#train acc
mean(lda.class[train]==cleandfcorr$ViolentCrimesFactor[train])
#test acc
mean(lda.class[-train]==cleandfcorr$ViolentCrimesFactor[-train])


qda.fit <- qda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47],subset=train)
#qda.fit
qda.class <- predict(qda.fit,cleandfcorr[,3:47])$class
table(qda.class,cleandfcorr$ViolentCrimesFactor)

#train acc
mean(qda.class[train]==cleandfcorr$ViolentCrimesFactor[train])
#test acc
mean(qda.class[-train]==cleandfcorr$ViolentCrimesFactor[-train])

```
 
```{r}
#using train and test
lreg.out1<-glm(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47], subset = train)

logistic.prob <- predict(lreg.out1, type="response") # want probability
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.71] <- "Low"
table(logistic.pred, cleandfcorr$ViolentCrimesFactor)

#train acc
mean(logistic.pred[train]==cleandfcorr$ViolentCrimesFactor[train])
#test acc
mean(logistic.pred[-train]==cleandfcorr$ViolentCrimesFactor[-train])

```


```{r}
# prova con il log
plot_usmap(data = vc_state, values = "logViolentCrimesMean", color = "black") +
  scale_fill_continuous(low = Rgb[3], high = Rgb[1], name = "log(Violent Crimes +1)",label=scales::comma) +
  theme(legend.position = "right")

# con log fa schifo, con la radice meglio...
plot_usmap(data = vc_state, values = "sqrtViolentCrimesMean", color = "black") +
  scale_fill_continuous(low = Rgb[3], high = Rgb[1], name = "sqrt(Violent Crimes)",label=scales::comma) +
  theme(legend.position = "right")
```

## Altri Plot a caso
### State Aggregation - data analysis

Maps plot on mean values per state ... vogliamo in caso tenerne qualcuno?

```{r, include=FALSE}
#plot mean some predictor and all response vars per state

dataf=cleandf


for(i in c(3,13,26,29,44, seq(dim(dataf)[2]-16,dim(dataf)[2]-2, by=2),dim(dataf)[2]-1,dim(dataf)[2])) {
  dataaggr <- aggregate(dataf[, colnames(dataf)[i]], list(dataf$state), mean)
  names(dataaggr)[names(dataaggr) == "Group.1"] <- "state"
  names(dataaggr)[names(dataaggr) == "x"] <- colnames(dataf)[i]
  
  mycol=c("#00AFBB", "#E7B800", "#FC4E07")[i%%3 + 1]


  pl<-plot_usmap(data = dataaggr, values = colnames(dataf)[i], color = 1) + 
    scale_fill_continuous(low = "white", high= mycol, name = colnames(dataf)[i], label = scales::comma) + 
    theme(legend.position = "right")
  
  print(pl)

}
```

### Pairs plot


```{r}
# pairs plot prova
pairs(~ medIncome+racepctblack+racePctAsian+racePctWhite+ViolentCrimesPerPop,data=cleandf)
```

```{r}
#scatter plot between all predictor vs ViolentCrimesPerPop (tantii)
par(mfrow=c(2,2))

#dataf=cleandf
#seq(3,dim(dataf)[2]-18)

#for(i in seq(3,dim(dataf)[2]-18)) {
  
  #scatter.smooth(dataf[,i],dataf$ViolentCrimesPerPop, xlab=colnames(dataf[i]), ylab='ViolentCrimesPerPop')

#}

par(mfrow=c(1,1))

```



```{r}
reg.pc.best <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores[,1:12]))

se  <- summary(reg.pc.best)$sigma                  # se
rsq <- summary(reg.pc.best)$r.squared              # R^2
adrsq <- summary(reg.pc.best)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(reg.pc.best)
par(mfrow=c(1,1))
```


```{r}
reg.pc <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores))
#summary(reg.pc)

se  <- summary(reg.pc)$sigma                  # se
rsq <- summary(reg.pc)$r.squared              # R^2
adrsq <- summary(reg.pc)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(reg.pc)
par(mfrow=c(1,1))
```

```{r}
reg.pc1 <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores[,1:15]))
#summary(reg.pc)

se  <- summary(reg.pc1)$sigma                  # se
rsq <- summary(reg.pc1)$r.squared              # R^2
adrsq <- summary(reg.pc1)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(reg.pc1)
par(mfrow=c(1,1))
```

```{r}
reg.pc2 <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores[,1:10]))
#summary(reg.pc2)

se  <- summary(reg.pc2)$sigma                  # se
rsq <- summary(reg.pc2)$r.squared              # R^2
adrsq <- summary(reg.pc2)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(reg.pc2)
par(mfrow=c(1,1))
```


```{r}
lreg.out<-glm(df$ViolentCrimesPerPop~., family = binomial, data=as.data.frame(pc$scores[,-c(11,12,14,18,19,23,26,27,seq(30,45))]))

summary(lreg.out)


# check the coding of ViolentCrimesFactor
contrasts(df$ViolentCrimesPerPop)

logistic.prob <- predict(lreg.out, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(df$ViolentCrimesPerPop, logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.67] <- "Low"

table(logistic.pred, df$ViolentCrimesPerPop)

#train error
(113+224)/1996

```

```{r}
lreg.out1<-glm(ViolentCrimesPerPop ~ ., family = binomial, data=corrdf)
summary(lreg.out)

logistic.prob <- predict(lreg.out1, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(corrdf$ViolentCrimesPerPop , logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.71] <- "Low"

table(logistic.pred, corrdf$ViolentCrimesPerPop)

#train error
(224+86)/1996

#train acc
mean(logistic.pred==corrdf$ViolentCrimesPerPop)

```


# Utili da tenere come esempi a voce nel caso chiedesse di piÃ¹ sulla features selection

* raceP(p)ct(...) columns sum up to more than 1, one hot encoding???
* numbUrban or pctUrban keep only one (maybe the second, so that we can eventually discard population)
  same for  NumUnderPov & PctPopUnderPov
            NumKidsBornNeverMar & PctKidsBornNeverMar (total missing)
            HousVacant & PctHousOccup & PctHousOwnOccup
            OwnOccLowQuart & OwnOccMedVal & OwnOccHiQuart & OwnOccQrange
            RentQrange (difference between 2 prev cols)
            Lemas(...) ALREADY REMOVED in cleandf
            PopDens (again, delete pop or similar)
            
            ?Target? & ?Target?PerPoP
* medIncome and medfamincome so similar
* PctImmigRec(...): compress in fewer columns with the coefficients describing the evolution ??? (obviously increasing (inclusion): 
at what rate?)
  same for PctBornSameState & PctSame(...)
* PctRec(...): same but over the full population

```{r}
# 0.9
# OK perCapInc del. vs whitePerCap (21 del. vs 22, use average)
# OK PctBSorMore kept vs PctOccupMgmtProf (32 kept vs 38, solved by 0.8)
# OK NumKidsBornNeverMar del. vs NumImmig (50 del. vs 52, solved by 0.8)
# 0.8
# OK racepvtblack vs racePctWhite (3 kept vs 4, let's keep minorities)
# OK agePct12t21 kept vs agePct12t29 (7 kept vs 8, solved by 0.7)
# OK medIncome del. vs whitePerCap (13 del. vs 22, use average)
# OK pctWPubAsst del. vs PctUnemployed (18 del. vs 33, solved by 0.7)
# OK PctPopUnderPov del. vs PctHousNoPhone (29 del. vs 78, solved by 0.7)
# OK PctYoungKids2Par del. vs PctTeen2Par (46 del. vs 47, solved by 0.7)
# OK PctWorkMomYoungKids kept vs PctWorkMom (48 kept vs 49, unanimity)
# OK PctImmigRecent kept vs PctImmigRec8 (53 kept vs 55)
# OK PctSameHouse85 kept vs PctSameCity85 (94 kept vs 95, more informative)
# 0.7
# OK NumImmig and HousVacant (52 and 72, unanimity)
```