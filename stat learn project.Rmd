---
title: "Communities and Crime"
subtitle: "Statistical Learning Final Exam Project"
author: "Caria Natascia, Cozzolino Claudia and Petrella Alfredo"
date: "26 maggio 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

```{r , include=FALSE}
library(car)

library(ggplot2)

# install.packages("usmap")

library(usmap)

# install.packages("varhandle", dependencies = T)
library(varhandle)

library(pROC)

library(MASS)

library(class)

# install.packages('corrplot')
library(corrplot)

# install.packages('factoextra')
library(factoextra)

# install.packages("ISLR")
library(ISLR)

library(glmnet)

library(leaps)

# install.packages("caret", dependencies = T)
library(caret)

set.seed(1234)
```

# Aim of the project

Predicting the rate of violent crime in a community can be particularly helpful in reducing the actual possibility of such crimes occurring.
Socio-economic, environmental and demographic characteristics can be important predictors of the level of violent crime in a population.
Therefore, determining which factors are the most influential can play a key role in understanding this complex phenomenon of crime.

This is what we are going to do with this project, by investigating the USA Communities and Crime Data Set, sourced from the [UCI Dataset Repository](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized).

# Dataset Description

The dataset contains a total number of 2215 instances, each one associated to a community in the US, and 147 attributes which result from the combination of socio-economic data from 1990 US Census, law enforcement data from the 1990 US LEMAS survey and crimedata from the 1995 FBI UCR.

# Data Cleaning

```{r}
# import dataset
crimedata <- read.csv("crimedata.csv", na.strings="?") # missing values are stored as "?"
dim(crimedata)
```

Attributes information:

* 4 non-predictive (communityname, countyCode, communityCode, fold)
* 125 predictive
* 18 potential response (murders, murdPerPop, rapes, rapesPerPop, robberies, robbbPerPop, assaults, assaultPerPop, burglaries, burglPerPop, larcenies, larcPerPop, autoTheft, autoTheftPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop)

From UCI description we know that the variables communityname and state are nominal while the remaining are all numeric.

```{r}
# check if variables communityname and state are stored as factors
is.factor(crimedata$communityname)
is.factor(crimedata$state)

# check number of numeric variables: 145 (= 147 - 2) expected
sum(sapply(crimedata, is.numeric))
```

## Missing values

```{r}
# inspect total NA
sum(is.na(crimedata))
mean(is.na(crimedata))
```

More than 10% of values is missing.

```{r, include=FALSE}
# install.packages("varhandle", dependencies = T)
library(varhandle)
```

```{r}
# nadf dataframe with columns that present NA 
nadf <- inspect.na(crimedata, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=TRUE, na.value = NA)
```

There are 41 columns with missing values, many of them with more than 50% of the data missing.
Among them:

* 3 non-predictive (countyCode, communityCode, fold);
* many variables from US LEMAS dataset, including policing information;
* potential response (rapes, rapesPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop in particular)

From documentaion:

*The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of rapes. These resulted in missing values for rape, which resulted in missing values for per capita violent crime. Many of these omitted communities were from the midwestern USA (Minnesota, Illinois, and Michigan have many of these).*

*The per capita nonviolent crime variable was calculated using the sum of crime variables considered non-violent crimes in the United States: burglaries, larcenies, auto thefts and arsons. (There are many other types of crimes, these only include FBI 'Index Crimes').*

Looking to the dataset, other suspecious values equal to 0.00 have been found:

```{r}
zero_count <- function(x) sum(x==0, na.rm = TRUE)
zerodf <- lapply(crimedata, zero_count)[lapply(crimedata, zero_count)!=0]
names(zerodf)
```

Although it makes sense for some of these variables to take zero values, for others it seems unrealistic. However, we do not have any information about it in the documentation.

Therefore, columns with more than 50% NA are dropped and so are the variables fold (not useful), OwnOccQrange and RentQrange (obtained as the difference of other columns). We decide to keep the other columns with zero values due to their meaningfulness.

```{r}
# find columns with > 50% NA
coltodrop <- as.vector(nadf$column_name[nadf$ratio_of_NA>0.50])

# add other columns to remove
coltodrop <- c(coltodrop, "fold", "OwnOccQrange", "RentQrange")
coltodrop

# drop columns with > 50% NA and clearly redundant ones
cleandf <- crimedata[,!(names(crimedata) %in% coltodrop)]
```

### Possible ways to handle missing values:

The remaining variables with missing values are the ones related to the crimes and another one, the variable OtherPerCap (per capita income for people with 'other' heritage), which has only one missing value.

```{r}
# remaining columns with NA
nadf <- inspect.na(cleandf, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=FALSE, na.value = NA)
nacolnames <- as.vector(nadf$column_name)
nacolnames
```

Possible ways to handle the remaining missing values:

1. drop all raws with at least one missing value;
2. substitute a missing value with the average computed over the state a community belongs;
3. leave NA value and consider it as another category.

```{r}
# rows with NA
narows <- inspect.na(cleandf, hist=FALSE, summary=TRUE, byrow=TRUE, barplot=FALSE, na.value = NA)
dim(narows)[1]
```

Since there are a lot of rows that have at least one missing values, we decide to proceed as in point 2.

```{r}
# dataframe of the columns which still contain NA with the mean computed over the state
meandf <- aggregate(cleandf[,nacolnames], list(cleandf$state), function(x) mean(x, na.rm = TRUE))
sum(is.na(meandf))
```

The dataframe with the mean contains NaN values, this means that there are States for which the value of a certain feature is zero for all communities that belong to them.
This happens for IL, MI (no data of rapes, rapesPerPop, ViolentCrimesPerPop), KS, VT (no data of arsons, arsonsPerPop, nonViolPerPop).
Moreover from documentation we know that also MN has a lot of missing values for rapes (59 out of 66):

```{r}
mnNA = sum(is.na(cleandf[cleandf$state=="MN","rapes"]))
mnTot = length(cleandf[cleandf$state=="MN","rapes"])
c(mnNA, mnTot, mnNA/mnTot)
```

Therefore, data related to those states is removed.

```{r}
rowtodrop <- as.numeric(rownames(cleandf[cleandf$state %in% c("IL","MI","MN","KS","VT"),]))

cleandf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]
```

For the remaining variables with missing values, we substitute NA with the mean computed over the state.

```{r}
# substitute a missing value with the average computed over the state
for(col in nacolnames) {
  match_table <- tapply(cleandf[[col]], cleandf$state, mean, na.rm=TRUE)
  NA_position <- which(is.na(cleandf[[col]]))
  cleandf[[col]][NA_position] <- match_table[cleandf$state[NA_position]]
}
```

Now cleandf doeas not contain NA.

```{r}
sum(is.na(cleandf))
dim(cleandf)
```

The resulting dataframe consists of 1996 instances and 120 attributes.

## Standardization

To avoid any bias due to the difference of the predictors content, the dataset must be somehow scaled.
Different methods have been tested because of the presence of several outliers, such as the classical mean-standard
deviation stardardization and the min-max normalization. In the latter case most of the information dropped lost for the previously
mentioned thickness of the tails of the columns densities, while we'll see that the former one, together with a logarithmic
transformation, gives far better results.

It is interesting to notice an additional detail: many columns of the dataset describe the same quantity for different categories.
An attempt has been to jointly standardize such groups of variables, considering common values for the mean and the standard deviation,
but the following results where basically equivalent to the first ones, so we decided not to overcomplicate the analysis.

```{r} 
standardization <- function(x) {
return ((x - mean(x)) / sd(x))
}

standf <- cleandf
standf[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], standardization)
```

```{r, include=FALSE}
# remove all variables from environment excpet useful dataset
rm(list=setdiff(ls(), c("crimedata", "cleandf", "standf")))
```

```{r, include=FALSE}
# save cleandf and standf to CSV 
# write.csv(cleandf, "crimedata-cleaned.csv", row.names=TRUE)
# write.csv(standf, "crimedata-cleaned-stand.csv", row.names=TRUE)
```

```{r, include=FALSE}
Col = c("#EF476F", "#FFD166", "#06D6A0", "#118AB2", "#073B4C")
Rgb = c(rgb(0.94, 0.28, 0.44, 0.7),
        rgb(1, 0.82, 0.4, 0.7),
        rgb(0.02, 0.84, 0.63, 0.7),
        rgb(0.07, 0.54, 0.7, 0.7),
        rgb(0.03, 0.23, 0.3, 0.7))
```

# Exploratory Data Analysis 

```{r}
# import cleaned dataset
# cleandf <- read.csv("crimedata-cleaned.csv", row.names = 1)
# dim(cleandf)
```

```{r}
# Violent Crimes per 100K Population - State aggregation - Mean
vc_state <- aggregate(cleandf[, "ViolentCrimesPerPop"], list(cleandf$state), mean)
names(vc_state)[names(vc_state) == "Group.1"] <- "state"
names(vc_state)[names(vc_state) == "x"] <- "ViolentCrimesMean"
```

First of all we try to understand if the phenomenon of crime occurs equally in different American states.

```{r}
summary(vc_state$ViolentCrimesMean)
```

The summary suggests that the distrubution is right skewed. Let us check it with some plots.

```{r}
par(mfrow=c(1,2))
hist(cleandf$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes",
     xlab = "Violent Crimes per 100K popuation")
lines(density(cleandf$ViolentCrimesPerPop))

boxplot(cleandf$ViolentCrimesPerPop, col = Col[1], main="Violent Crimes")

par(mfrow=c(1,1))
```

```{r, include=FALSE}
library(ggplot2)
library(usmap)
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = vc_state, values = "ViolentCrimesMean", color = "black") +
   scale_fill_continuous(trans="sqrt", low = Rgb[3], high = Rgb[1], name = "Violent Crimes",limits=c(85,3049),
label=scales::comma)+
  theme(legend.position = "right")

```

```{r}
violent_names <- c("murders", "rapes", "robberies", "assaults")
violPerPop_names <- c("murdPerPop", "rapesPerPop", "robbbPerPop", "assaultPerPop")

violent <- cleandf[, (colnames(cleandf) %in% violent_names)]
violPerPop <- cleandf[, (colnames(cleandf) %in% violPerPop_names)]
```

```{r}
par(mfrow = c(2,2))
boxplot(violent, outline = TRUE, col = Col[seq(1:4)])
boxplot(violent, outline = FALSE, col = Col[seq(1:4)])

boxplot(violPerPop, outline = TRUE, col = Col[seq(1:4)])
boxplot(violPerPop, outline = FALSE, col = Col[seq(1:4)])
par(mfrow=c(1,1))
```

```{r}
# Non Violent Crimes per 100K Population - State aggregation - Mean
nvc_state <- aggregate(cleandf[, "nonViolPerPop"], list(cleandf$state), mean)
names(nvc_state)[names(nvc_state) == "Group.1"] <- "state"
names(nvc_state)[names(nvc_state) == "x"] <- "NonViolentCrimesMean"
```

```{r}
summary(nvc_state$NonViolentCrimesMean)
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = nvc_state, values = "NonViolentCrimesMean", color = "black") +
   scale_fill_continuous(trans="sqrt",low = Rgb[3], high = Rgb[1], name = "Non Violent Crimes",limits=c(2825,9252),
label=scales::comma)+
  theme(legend.position = "right")
```

```{r}
nonViol_names <- c("burglaries", "larcenies", "autoTheft", "arsons")
nonViolPerPop_names <- c("burglPerPop", "larcPerPop", "autoTheftPerPop", "arsonsPerPop")

nonViol <- cleandf[, (colnames(cleandf) %in% nonViol_names)]
nonViolPerPop <- cleandf[, (colnames(cleandf) %in% nonViolPerPop_names)]
```

```{r}
par(mfrow = c(2,2))
boxplot(nonViol, outline = TRUE, col = Col[seq(1:4)])
boxplot(nonViol, outline = FALSE, col = Col[seq(1:4)])

boxplot(nonViolPerPop, outline = TRUE, col = Col[seq(1:4)])
boxplot(nonViolPerPop, outline = FALSE, col = Col[seq(1:4)])
par(mfrow=c(1,1))
```

Let us analyze which variables are more correlated (correlation>0.5) with the variable of violent crimes.

```{r}
coltodrop <- c(1,2, seq(103,118), 120)
viol.corrdf <- cleandf[,-coltodrop]

# correlation matrix
viol.cm <- cor(viol.corrdf, use='complete.obs')

# violent crimes correlation
ViolCrimes.corr <- viol.cm[,dim(viol.cm)[2]]
ViolCrimes.names <- names((ViolCrimes.corr)>0.5)

sort(ViolCrimes.corr[abs(ViolCrimes.corr)>0.5])
```

It seems that the variables more correlated with violent crimes are also correlated with each other. For instance, the attributes PctFam2Par, PctYoungKids2Par, PctKids2Par, PctTeen2Par all refer to the family structure, in particular to the percentage of families (with kids with different age) that are headed by two parents. Probably, also the variables TotalPctDiv, FemalePctDiv and MalePctDiv in addition to being trivially correlated with each other, are correlated with the previous ones.
In general we can say that the family structure, the marital status, the economic status ( pctWInvInc, PctPopUnderPov, PctWPubAsst) and the ethnicity (racepctWhite, racepctblack) are 4 main clusters stricly correlated with Violent Crimes.

Let us investigate if and how these clusters are cross-correlated through a scatterplot.

```{r}
pairs(cleandf[,colnames(cleandf)%in% ViolCrimes.names], pch = 20)
```

The plot confirms what already said above.
Moreover, we can observe that there exists also a relationship between ethnicity and family structure:
* there is a negative correlation between the percentage of African-American and the percentage of families (with kids differently aged) headed by 2 parents;
* there is a strong positive correlation between the percentage of African-American and the percentage of kids born from never married parents.

```{r}
african.corr <- viol.cm[,3]
sort(african.corr[abs(african.corr)>0.5])
```

We carry now the same analysis in order to understand which variables are more correlated (correlation>0.5) with the variable of non violent crimes.

```{r}
coltodrop <- c(1,2, seq(103,119))
nonViol.corrdf <- cleandf[,-coltodrop]

# correlation matrix
nonViol.cm <- cor(nonViol.corrdf, use='complete.obs')

# violent crimes correlation
nonViol.corr <- nonViol.cm[,dim(nonViol.cm)[2]]

sort(nonViol.corr[abs(nonViol.corr)>0.5])
```

We can observe that the most correlated variables are still those related to the family structure, the marital status and poverty.

```{r, include=FALSE}
# remove no more useful variables
rm(list=setdiff(ls(), c("crimedata", "cleandf", "standf", "Col")))
```



# Multiple linear regression

response = ViolentCrimesPerPop (forcing the assumpition of equal mean between state t)

Remove non predictive variables:
communityname, state, all crimes except variable ViolentCrimesPerPop
```{r}
coltodrop <- c(1,2, seq(103,120)[-17]) # -17 keeps ViolentCrimesPerPop

df <- standf[,-coltodrop]
```

Multiple Linear Regression
```{r}
reg.out <- lm(ViolentCrimesPerPop~., data=df)
```

```{r}
se  <- summary(reg.out)$sigma                  # se
rsq <- summary(reg.out)$r.squared              # R^2
adrsq <- summary(reg.out)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out)
par(mfrow=c(1,1))
```

## Assessing Model Assumptions:
The model assumptions are:

- Linearity of the response-predictor relationships;
- Homeschedasticity: Var($\epsilon_i$) = $\sigma^2$;
- Normality and Independence of $\epsilon_i$.

### Linearity
The plots of residuals versus fitted values shows a little pattern, however it seems to indicate that there are linear associations in
the data. (e ci dice che gli errori sono non correlati :))

### Homoschedasticity
The presence of a funnel shape in the residual plot suggests that the error terms do not have a constant variance. One possible
solution is to transform the response variable $Y$ using a concave function such as $\log(Y)$ or $\sqrt(Y)$ .




```{r}
par(mfrow=c(1,2))
hist(cleandf$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes")
lines(density(cleandf$ViolentCrimesPerPop))

hist(log(cleandf$ViolentCrimesPerPop +1), prob=TRUE, breaks=30, col=Col[1], main="log(Violent Crimes +1)")
lines(density(log(cleandf$ViolentCrimesPerPop +1)))
par(mfrow=c(1,1))


par(mfrow=c(1,2))
hist(standf$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes")
lines(density(standf$ViolentCrimesPerPop))

hist(log(standf$ViolentCrimesPerPop +1), prob=TRUE, breaks=30, col=Col[1], main="log(Violent Crimes +1)")
lines(density(log(standf$ViolentCrimesPerPop +1)))
par(mfrow=c(1,1))

```

```{r}
par(mfrow=c(1,2))
qqnorm(cleandf$ViolentCrimesPerPop, main="Violent Crimes")
qqline(cleandf$ViolentCrimesPerPop)

qqnorm(log(cleandf$ViolentCrimesPerPop +1), main="log(Violent Crimes +1)")
qqline(log(cleandf$ViolentCrimesPerPop +1))
par(mfrow=c(1,1))

par(mfrow=c(1,2))
qqnorm(standf$ViolentCrimesPerPop, main="Violent Crimes")
qqline(standf$ViolentCrimesPerPop)

qqnorm(log(standf$ViolentCrimesPerPop +1), main="log(Violent Crimes +1)")
qqline(log(standf$ViolentCrimesPerPop +1))
par(mfrow=c(1,1))

```


```{r}
reg.out2 <- lm(log(ViolentCrimesPerPop+1)~., data=df)
```

```{r}
se  <- summary(reg.out2)$sigma                  # se
rsq <- summary(reg.out2)$r.squared              # R^2
adrsq <- summary(reg.out2)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2)) 
plot(reg.out2)
par(mfrow=c(1,1))
```

The plots of residuals versus fitted values shows that such a transformation leads to a reduction in heteroscedasticity.


### Normality and Independence of errors
We assess normality looking to the QQplot....


## Other Possible Problems

### Outliers
The residual plot identifies some outliers. However, it can be difficult to decide how large a residual needs to be before we consider
the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed
by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in
absolute value are possible outliers.

Note: Why 3? The Studentized Residuals are approximated by a N(0,1).
The probability to observe a value greater than 3 is 0.001349898. [ref biblio? Per me sì (Alfredo)]

```{r}
1 - pnorm(3)
```

```{r}
plot(predict(reg.out2), rstandard(reg.out2), xlab="Fitted Values", ylab = "Studentized Residuals")

abline(h=3, col = "red")
abline(h=-3, col = "red")
```
Explore and interpret city of outliers...? (dell'est strano.. commenta)

```{r}
out <- names(rstandard(reg.out2)[(abs(rstandard(reg.out2)) > 3)])

standf[out,c(1,2)]

cityout<-standf$communityname[rownames(standf) %in% out]
```


```{r}
citycord <- read.csv("cities.csv")

# Rename column 
colnames(citycord)[colnames(citycord) == "city"] <- "communityname"
colnames(citycord)[colnames(citycord) == "latitude"] <- "lat"
colnames(citycord)[colnames(citycord) == "longitude"] <- "lon"


citycord<-merge(citycord,cleandf,by=c("state","communityname"))
```


```{r}
dfplot <- citycord[citycord$communityname %in% cityout, c("lon","lat","ViolentCrimesPerPop","communityname")]
```


```{r}

cities_t <- usmap_transform(dfplot)

#without city labels
plot_usmap(fill = "#06D6A0", alpha = 0.25) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = ViolentCrimesPerPop),
             color = "#073B4C", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Outliers communities",
       size = "Violent Crimes per 100 K inhabitants") +
  theme(legend.position = "right")

#with city labels
plot_usmap(fill = "#06D6A0", alpha = 0.25) +
  ggrepel::geom_label_repel(data = cities_t,
             aes(x = lon.1, y = lat.1, label = communityname),
             size = 2.5, alpha = 0.8,
             label.r = unit(0.5, "lines"), label.size = 0.55,
             segment.color = "black", segment.size = 0.7,
             seed = 1002) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = ViolentCrimesPerPop),
             color = "#073B4C", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Outliers communities",
       size = "Violent Crimes per 100 K inhabitants") +
  theme(legend.position = "right")

```


```{r}
# regression without outliers
reg.out3 <- lm(log(ViolentCrimesPerPop+1)~., data=df[!(rownames(df) %in% out),])
```

```{r}
se  <- summary(reg.out3)$sigma                  # se
rsq <- summary(reg.out3)$r.squared              # R^2
adrsq <- summary(reg.out3)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2)) 
plot(reg.out3)
par(mfrow=c(1,1))
```

The RSE is 0.56 when the outlier is included in the regression and it is 0.52 when the outlier is removed. Since the RSE is used to
compute all confidence intervals and p-values, this can have implications for the interpretation of the fit. Moreover, dropping the
outliers the $R^2$ increses from 0.67 to 0.7. [??? A me vengono 0.68 e 0.71, da ricontrollare alla fine]
Care should be taken in the decision of taking or dropping outliers.

### High Leverage Points
In order to quantify an observations leverage, we compute the leverage statistic. If a given observation has a leverage statistic that
greatly exceeds (p+1)/n, then we may suspect that the corresponding point has high leverage.

```{r}
hv <- hatvalues(reg.out2)
plot(hv, rstandard(reg.out2), xlab="Leverage", ylab = "Studentized Residuals")

p <- dim(df)[2]-1
n <- dim(df)[1]
abline(v=(p+1)/n, col = "red")
```


Try removing high leverage points [ref biblio]
explore city state leverage interpret?

```{r}
lev <- names(hv[hv>3*(p+1)/n])
standf[lev,c(1,2)]

citylev<-standf$communityname[rownames(standf) %in% lev]
```

```{r}
dfplot<-citycord[citycord$communityname %in% citylev,c("lon","lat","communityname","state","population")]
```

```{r}

cities_t <- usmap_transform(dfplot)


#without city labels
plot_usmap(fill = "#f26b8b", alpha = 0.23) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = population),
             color = "#a7314d", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Leverage communities",
       size = "Total community population") +
  theme(legend.position = "right")

#with city labels
plot_usmap(fill = "#f26b8b", alpha = 0.23) +
  ggrepel::geom_label_repel(data = cities_t,
             aes(x = lon.1, y = lat.1, label = communityname),
             size = 2, alpha = 0.8,
             label.r = unit(0.5, "lines"), label.size = 0.01,
             segment.color = "black", segment.size = 0.5,
             seed = 1002) +
  geom_point(data = cities_t,
             aes(x = lon.1, y = lat.1, size = population),
             color = "#a7314d", alpha = 0.5) +
  scale_size_continuous(range = c(1, 16),
                        label = scales::comma) +
  labs(title = "Leverage communities",
       size = "Total community population") +
  theme(legend.position = "right")

```

```{r}
# regression without leverage points
reg.out4 <- lm(log(ViolentCrimesPerPop+1)~., data=df[!(rownames(df) %in% lev),])

se  <- summary(reg.out4)$sigma                  # se
rsq <- summary(reg.out4)$r.squared              # R^2
adrsq <- summary(reg.out4)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out4)
par(mfrow=c(1,1))
```

Con questo cut non sembra esserci molta differenza... ma RSE diminuisce di qualche millesimo.. 
e togliamo solo 56 righe... leggo dalla source sopra la seguente...:
a high leverage observation may or may not actually be influential, a data point has large influence only if it affects the estimated
regression function.. potrebbe essere una buona notizia :)




### Collinearity 
Collinearity refers to the situation in which two or more predictor variables
collinearity
are closely related to one another. The presence of collinearity can pose problems in
the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.

Since collinearity reduces the accuracy of the estimates of the regression
coefficients, it causes the standard error for $\hat \beta_j$ to grow. Recall that the
t-statistic for each predictor is calculated by dividing $\hat \beta_j$ by its standard
error. Consequently, collinearity results in a decline in the t-statistic. As a
result, in the presence of collinearity, we may fail to reject $H_0:\ \beta_j = 0$. This
means that the power of the hypothesis test-the probability of correctly
power
detecting a non-zero coefficient-is reduced by collinearity. 

To avoid such a situation, it is desirable to identify and address potential
collinearity problems while fitting the model.
A simple way to detect collinearity is to look at the correlation matrix
of the predictors.

#### Correlation insight

In the following few lines of code we perform a first manual skimming of strongly dependent variables removing, between the most
correlated ones, the more redundant and meaningless.

```{r}
# restrict to the predictive numeric attributes
coltodrop <- c(1,2, seq(103,120))
corrdf <- cleandf[,-coltodrop]

# correlation matrix
cm <- cor(corrdf, use='complete.obs')

par(mfrow=c(1,2))

# correlation matrix plot
corrplot(cm,title='Initial correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35) # order=!!!

# correlation tradeoff
threshold <- 0.7
# only strongly correlated attributes highlighted
cma <- abs(cm) > threshold
# number of strong correlations
(sum(cma) - dim(corrdf)[2]) / 2

# filtered correlation matrix plot
corrplot(cma,title='Filtered correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

par(mfrow=c(1,1))
```

??? Spiega le scelte e soprattutto quella a 0.7!

```{r}
# columns with correlation with more meaningful ones higher than threshold in absolute value

rem9 <- c( 
"population","agePct16t24","numbUrban","pctWSocSec","medFamInc","perCapInc",
"NumUnderPov","PctLess9thGrade","PctOccupMgmtProf","MalePctDivorce","FemalePctDiv","PctFam2Par",
"PctKids2Par" ,"NumKidsBornNeverMar", "PctImmigRec5","PctImmigRec10",      
"PctRecImmig5","PctRecImmig8","PctRecImmig10","PctSpeakEnglOnly" ,  
"PctLargHouseOccup","PersPerOccupHous","PctHousOwnOcc","OwnOccLowQuart",     
"OwnOccHiQuart","RentLowQ","RentHighQ","MedRent",            
"NumInShelters","NumStreet","PctForeignBorn")

rem8 <- c(rem9,
"householdsize","racePctWhite","agePct12t29","medIncome",
"pctWWage","pctWPubAsst","PctPopUnderPov","PersPerFam",         
"PctYoungKids2Par","PctWorkMom","PctKidsBornNeverMar", "PctImmigRec8",       
"PctNotSpeakEnglWell", "PctPersDenseHous","OwnOccMedVal","RentMedian",         
"PctSameCity85")

rem7 <- c(rem8,
"pctWInvInc","PctNotHSGrad","PctBSorMore","PctLargHouseFam","PctHousLess3BR", 
"MedNumBR","PctSameState85" )


corrdf_ind <- corrdf[,!(colnames(corrdf) %in% rem8)]

# final correlation matrix and filter
cm_ind <- cor(corrdf_ind, use='complete.obs')
cma_ind <- abs(cm_ind) > threshold

par(mfrow=c(1,1))
# resulting filtered correlation matrix plot
corrplot(cma_ind, title='Filtered correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

# 0.9
# OK perCapInc del. vs whitePerCap (21 del. vs 22, use average)
# OK PctBSorMore kept vs PctOccupMgmtProf (32 kept vs 38, solved by 0.8)
# OK NumKidsBornNeverMar del. vs NumImmig (50 del. vs 52, solved by 0.8)
# 0.8
# OK racepvtblack vs racePctWhite (3 kept vs 4, let's keep minorities)
# OK agePct12t21 kept vs agePct12t29 (7 kept vs 8, solved by 0.7)
# OK medIncome del. vs whitePerCap (13 del. vs 22, use average)
# OK pctWPubAsst del. vs PctUnemployed (18 del. vs 33, solved by 0.7)
# OK PctPopUnderPov del. vs PctHousNoPhone (29 del. vs 78, solved by 0.7)
# OK PctYoungKids2Par del. vs PctTeen2Par (46 del. vs 47, solved by 0.7)
# OK PctWorkMomYoungKids kept vs PctWorkMom (48 kept vs 49, unanimity)
# OK PctImmigRecent kept vs PctImmigRec8 (53 kept vs 55)
# OK PctSameHouse85 kept vs PctSameCity85 (94 kept vs 95, more informative)
# 0.7
# OK NumImmig and HousVacant (52 and 72, unanimity)

# final correlation matrix plot
corrplot(cm_ind, title='Final correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

par(mfrow=c(1,1))
```



When faced with the problem of collinearity, there are two simple solutions.
The first is to drop one of the problematic variables from the regression. 
This can usually be done without much compromise to the regression
fit, since the presence of collinearity implies that the information that this
variable provides about the response is redundant in the presence of the
other variables. The second solution is
to combine the collinear variables together into a single predictor.


```{r}
corrdf <- df[,!(colnames(df) %in% rem8)]

reg.out5 <- lm(log(ViolentCrimesPerPop+1)~., data=corrdf)
#summary(reg.out5)

se  <- summary(reg.out5)$sigma                  # se
rsq <- summary(reg.out5)$r.squared              # R^2
adrsq <- summary(reg.out5)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out5)
par(mfrow=c(1,1))
```
It is possible to notice that the model performs slightly worse than the previous one, both looking at the RSE and the adjusted $R^2$ but the dimensionality of the problem has been reduced of the 55%, and we still have other trump cards to improve it's effectiveness.


```{r, include=FALSE}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf","crimedata","standf","corrdf","Col")))
```


# Variables selection

With the removal of highly correlated columns, the dataset has decreased from 100 to 52 numeric predictor features. Even if we almost halved the dimension, p, the number of variables is still large.
As we will see, many of these variables are irrelevant and not associated with the response. Performing attribute selection is then a necessary step in order to find a good and representative subset of significative features, avoiding then to include useless information that makes our model unnecessarily complex.
In this section different techniques for feature selection are presented and tested such as exhaustive and greedy searches, PCA and shrinkage methods.


Before to go into the details, note that from now on the models are trained not on the full dataset but only on a fraction (80% of the rows). The remaining part is then used to estimate the test error. Results are given in terms of MSE (Mean Squared Error) and adjusted $R^2$ to ensure comparability between models built with different numbers of predictors.

## Train - Test split
```{r}
# Train-Test samples
train.sample <- as.numeric(sample(rownames(corrdf), 0.8*dim(corrdf)[1]))
test.sample <- as.numeric(setdiff(rownames(corrdf), train.sample))
```

```{r}
X = subset(corrdf, select = - c(ViolentCrimesPerPop))
X.train <- subset(X, (row.names(X) %in% train.sample))
X.test <- subset(X, (row.names(X) %in% test.sample))

Y <- log(corrdf["ViolentCrimesPerPop",drop=FALSE] +1)

Y.train <- unlist(subset(Y, (row.names(Y) %in% train.sample)))
Y.test <- unlist(subset(Y, (row.names(Y) %in% test.sample)))
```

We firstly compute the test error on the full model in order to have beseline values. For "full" here we mean the linear regression model using all the standardized columns left from the correlation investigation which aims to predict the log-transformed variable ViolentCrimesPerPop.

## Full model

```{r}
full.mod <- lm(Y.train~., data=X.train)
summary(full.mod)
```

```{r}
# prediction 
n <- 400
p <- 52
f.pred = predict(full.mod, X.test)
rss <- sum((Y.test - f.pred)^2)           # Residual Sum of Squares
ess <- sum((f.pred - mean(Y.test))^2)     # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

As expected, from the summary it can be noticed that many of the 52 predictors are not relevant for the prediction of the violent crimes rate, making us think that there exists a better subset of variables able to lower the MSE and increase the adjusted $R^2$.

## Best subset selection

The first approach we tried is the Best Subset Selection method, an intuitively simple strategy which fits all possible models with at most k = 1, 2, ..., p attributes chosen between all the p available and then select the best according to different measures such as $C_p$, BIC and adjusted $R^2$.
It is easy to understand that this exhaustive search has exponential time in p, making it not very efficient for our case. For this reason we performed the method fixing the maximum number of features to 15 instead of 52. Note that even with this big reduction the function is very slow ( 10 minutes running time). 


```{r}
regfit.full <- regsubsets(Y.train~., nvmax = 15, really.big=T, data= X.train)

reg.summary <- summary(regfit.full)
```

The plots of the evaluation measures $C_p$, BIC and adjusted $R^2$ in function of the number of variables are shown below with a red star in corrispondence of the optima.   

```{r}
# first group of plots 
#
par(mfrow=c(2,2))

# panel 1
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")

# panel 2
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
mad<-which.max(reg.summary$adjr2)
points(mad,reg.summary$adjr2[mad], col="red",cex=2,pch=20)

# panel 3
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
mcp<-which.min(reg.summary$cp)
points(mcp,reg.summary$cp[mcp],col="red",cex=2,pch=20)

# panel 4
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
mb<-which.min(reg.summary$bic)
points(mb,reg.summary$bic[mb],col="red",cex=2,pch=20)

par(mfrow=c(1,1))
```

In addition, graphical representations of the best subsect selected for each value of k $\in [1,15]$ are plotted. Note that the intensity of the colours on the gray scale corresponds to the level of significance.

```{r}
# second group of plots 

plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")

```

Finally, the best models selected according to $C_p$, BIC and adjusted $R^2$ the are then tested.
```{r}
# predict with adjR2 best
coef<-coef(regfit.full,mad) #coeff of the best model suggested by adjR2

#best subsect of var
as.data.frame(coef)

advar<-names(coef)[-1]

n <- 400
p <- mad
ad.pred=as.matrix(X.test[,advar])%*%coef[advar] + coef["(Intercept)"]
rss <- sum((Y.test - ad.pred)^2)       # Residual Sum of Squares
ess <- sum((ad.pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

```{r}
# predict with Cp best
coef<-coef(regfit.full,mcp) #coeff of the best model suggested by Cp 

#best subsect of var
as.data.frame(coef)

cpvar<-names(coef)[-1]

n <- 400
p <- mcp

cp.pred=as.matrix(X.test[,cpvar])%*%coef[cpvar] + coef["(Intercept)"]
rss <- sum((Y.test - cp.pred)^2)       # Residual Sum of Squares
ess <- sum((cp.pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

```{r}
# predict with BIC best
coef<-coef(regfit.full,mb) #coeff of the best model suggested by BIC 

#best subsect of var
as.data.frame(coef)

bvar<-names(coef)[-1]

n <- 400
p <- mb

b.pred=as.matrix(X.test[,bvar])%*%coef[bvar] + coef["(Intercept)"]
rss <- sum((Y.test - b.pred)^2)       # Residual Sum of Squares
ess <- sum((b.pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

The results are very good considering that we reduced p from 52 to at most 15, with all the three choices this simplified models perfom better than the full both in terms of MSE and adjusted $R^2$.

## Greedy search algorithms

The Best subset method is very time expensive when searching between large number of feautures as in our application, moreover only models with a small k can be investigated in feasible time. This limitations motivates us to abandone exact and exhaustive approaches and move to greedy searches like backward and forward selection.

### Backward selection

Backward selection begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-at-a-time.

```{r}
step.mod <- step(full.mod, steps=53,  trace=0, direction="backward")
summary(step.mod)
```

```{r}
# prediction 
n <- 400
p <- 31
bw.pred = predict(step.mod, X.test)
rss <- sum((Y.test - bw.pred)^2)           # Residual Sum of Squares
ess <- sum((bw.pred - mean(Y.test))^2)     # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

### Forward selection

Forward stepwise selection begins with the empty model containing no predictors, and then adds one-at-a-time the most significative predictor.

```{r}
empty.mod <- lm(Y.train~1, data=X.train)

step.mod <- step(empty.mod, steps=53,  trace=0, scope=list(lower=formula(empty.mod), upper=formula(full.mod)), direction="forward")
summary(step.mod)
```

```{r}
# prediction 
n <- 400
p <- 27
fw.pred = predict(step.mod, X.test)
rss <- sum((Y.test - fw.pred)^2)       # Residual Sum of Squares
ess <- sum((fw.pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

These greedy algorithms managed in few seconds to achieve quality solutions in terms of meaningfulness and reduction of the variables, MSE and adjusted $R^2$ scores. Note that the accuracy in MSE is equal to the one of super time expensive best subset methods, backward and forward strategy proved then to be very efficient in our quite large application.


## Principal Component Analysis


```{r}
# PCA computation
pc <- princomp(X) # cor=TRUE to obtain it from the correlation matrix

#str(pc)
# first k principal components
k <- 40
plot(pc, npcs=k, cex.names=0.5,las=2, col="#00AFBB", main='Principal Components')

par(mfrow=c(1,2))
barplot(pc$loadings[,1], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 1')
barplot(pc$loadings[,2], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 2')
barplot(pc$loadings[,3], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 3')
barplot(pc$loadings[,4], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 4')
barplot(pc$loadings[,5], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 5')
barplot(pc$loadings[,6], cex.names=0.5,las=+2, col=  "#FC4E07", main='Component 6')
barplot(pc$loadings[,7], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 7')
barplot(pc$loadings[,8], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 8')
barplot(pc$loadings[,9], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 9')
barplot(pc$loadings[,10], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 10')
par(mfrow=c(1,1))

#influent features per component 1 e 2
infl1 <- colnames(corrdf[,-dim(corrdf)[2]])[abs(pc$loadings[,1])>0.25]
infl1
infl2 <- colnames(corrdf[,-dim(corrdf)[2]])[abs(pc$loadings[,2])>0.2]
infl2


fviz_pca_var(pc,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T,     # Avoid text overlapping
             select.var = list(contrib = 15)
             )
```
Trying to interpret the main components of the first loading (corresponding to the maximum variance direction), it seems to be mainly identified by the original columns related to the working condition of the householder(s). It is interesting to notice the presence of the column "PctHousNoPhone", the percentage of occupied housing units without phone, but one should notice that in 1990 it was a strong indication of lack of need to contact and be contacted, besides the economical factor.


### PC regression

```{r, include=FALSE}
# install.packages("pls")
library(pls)
```

```{r}
pcr.fit = pcr(Y.train~., data=X.train, validation = "CV") # k=10 by default
# summary(pcr.fit)

# plot of CV error
validationplot(pcr.fit, val.type = "MSEP")
validationplot(pcr.fit, val.type = "R2")
# looking at the plot, 12 could be a great choice
```
A great tradeoff is to train a model using the first 12 principal components.
```{r}
# prediction in the model above
n <- 400
p <- 11
pcr.pred = predict(pcr.fit, X.test, ncomp=11)
rss <- sum((Y.test - pcr.pred)^2)          # Residual Sum of Squares
ess <- sum((pcr.pred - mean(Y.test))^2)    # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```



## Shrinkage methods

```{r}
# Convert dataframes to matrixes
X.train <- as.matrix(X.train)
X.test <- as.matrix(X.test)
Y.train <- as.matrix(Y.train)
Y.test <- as.matrix(Y.test)
```

### Ridge Regression with Cross-Validation
```{r}
# Lambda values to try
lambda.val <- 10^seq(3, -3, length=100)

# Ridge-Regression with 10-fold cross-validation on training set to choose lambda
ridge.mod.cv <- cv.glmnet(X.train, Y.train, alpha=0,
                          lambda=lambda.val, nfolds = 10)
plot(ridge.mod.cv)
```

```{r}
# Best cross-validated lambda
lam.min <- ridge.mod.cv$lambda.min
lam.1se <- ridge.mod.cv$lambda.1se
```

```{r}
# Ridge-Regression on the entire train set
ridge.mod <- glmnet(X.train, Y.train , alpha=0, lambda=lambda.val)
plot(ridge.mod, xvar = "lambda")
abline(v=log(lam.min), lty=2, col=1)
abline(v=log(lam.1se), lty=2, col=2)
legend("bottomright", c("lambda.min", "lambda.1se"), lty=c(2,2), col = c(1,2))
```
```{r}

ridge.coef <- as.matrix(coef(ridge.mod.cv,s=lam.min))
ridge.coef <- as.data.frame(ridge.coef)
colnames(ridge.coef) <- c("Coefficients")

# number of predictors
sum(abs(ridge.coef$Coefficients)>10^(-16))-1 #remove intercept

# Most relevant predictors
subset.data.frame(ridge.coef, abs(Coefficients)>0.05)
```

```{r}
# Prediction on test set
Y.hat <- predict(ridge.mod.cv, newx = X.test, s=lam.min)

n <- 400
p <- 52

rss <- sum((Y.test - Y.hat)^2)       # Residual Sum of Squares
ess <- sum((Y.hat - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```



### Lasso Regression with Cross-Validation
```{r}
# Lambda values to try
lambda.val <- 10^seq(3, -3, length=100)

# Lasso-Regression with 10-fold cross-validation on training set to choose lambda
lasso.mod.cv <- cv.glmnet(X.train, Y.train, alpha=1,
                          lambda=lambda.val, nfolds = 10)
plot(lasso.mod.cv)
```

```{r}
# Best cross-validated lambda
lam.min <- lasso.mod.cv$lambda.min
lam.1se <- lasso.mod.cv$lambda.1se
```

```{r}
# Ridge-Regression on the entire train set
lasso.mod <- glmnet(X.train, Y.train , alpha=1, lambda=lambda.val)
plot(lasso.mod, xvar = "lambda")
abline(v=log(lam.min), lty=2, col=1)
abline(v=log(lam.1se), lty=2, col=2)
legend("bottomright", c("lambda.min", "lambda.1se"), lty=c(2,2), col = c(1,2))
```

```{r}
lasso.coef <- as.matrix(coef(lasso.mod.cv,s=lam.min))
lasso.coef <- as.data.frame(lasso.coef)
colnames(lasso.coef) <- c("Coefficients")

# number of predictors
sum(abs(lasso.coef$Coefficients)>10^(-16))-1 #remove intercept

# Most relevant predictors
subset.data.frame(lasso.coef, abs(Coefficients)>0)
```

```{r}
# Prediction on test set
Y.hat <- predict(lasso.mod.cv, newx = X.test, s=lam.min)

n <- 400
p <- 48

rss <- sum((Y.test - Y.hat)^2)       # Residual Sum of Squares
ess <- sum((Y.hat - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```


# Other possible predictive Models

In this section other models have been tested for the prediction of the violent and non violent rate var variables.

## Add categorical predictor

Untill now we assumed that the response variable ViolentCrimesPerPop has equal mean and behaviour between the different states. Even if in practice this assumption considerably simplified the model, the hypothesis of similar distribution between different cities belonging to a vast territory such as the American one, characterized by a wide variety of geographic, cultural and social conditions, could be a too strong request.
Below a side by side boxplot of ViolentCrimesPerPop between the different states is presented (using the  standardized and log transformed version of the data). The horizontal line draws the mean value between all data without distinction.

```{r}
boxplot(log(standf$ViolentCrimesPerPop+1)~standf$state, xlab="state", ylab=expression(log(ViolentCrimesPerPop+1)), col= Col)
abline(h=mean(log(standf$ViolentCrimesPerPop+1)), col = "red")
```

The plot seems to confirm the doubts discussed above, therefore we tested the linear model considering the categorical variable State as factor.

```{r}
XS = subset(corrdf, select = - c(ViolentCrimesPerPop))
XS["state"]=standf$state

is.factor(XS$state) 

XS.train <- subset(XS, (row.names(XS) %in% train.sample))
XS.test <- subset(XS, (row.names(XS) %in% test.sample))


regc.out <- lm(Y.train~. , data=XS.train) #application of LR with a categorical variable
summary(regc.out)
```

From the results plotted by the summary, it seems that adding this information does not help too much the model, the State variable seem not very influential. The only exceptions are the Maine and North Dakota states (ME and ND) for which the information is more significant. Analyzing the position of these two territories it is interesting to note that they are both two northern states on the border with Canada, this precise geographical diversity could be the reason for our result.

Testing the model, as expected the MSE is slightly better than the equivalent model withot the categorical information, otherwise, the adjusted $R^2$ is smaller.


```{r}
# prediction 
pred = predict(regc.out, XS.test)

n <- 400
p <- 94

rss <- sum((Y.test - pred)^2)       # Residual Sum of Squares
ess <- sum((pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

Given the contradictory results between the boxplot and the model, we decided to conduct a deeper study by performing ANOVA, Tukey's test and ANCOVA on our dataset.
As expected, the ANOVA gives a very small p-value, confirming the theory of not all equal means. At this point hence the Tukey method for all pairwise mean comparisons is applied.

```{r}
summary(aov.state <- aov(ViolentCrimesPerPop ~ state, data = standf))
#reject all equal mean hyphothesis?

result<-TukeyHSD(aov.state, "state", ordered = TRUE)

result<-as.data.frame(result$state)

result[result[,4]<0.05,] #where reject equal mean hyphothesis?

```

Examining all the pairs in which Tukey's test allows to reject the hypothesis, i.e. where the adjusted p-value is less than $\alpha=0.05$, it can be concluded that the states can be grouped in two main cluster. Also in this case it is interesting to observe how the statistical test result can be interpreted geographically, the two clusers exactly divide the USin two regions, making us hypothesize that the variable of interest may have influences related to social and cultural aspects related to the position.

```{r}
cluster1 <- c("ND","ME","WI","NH","SD","UT","CT","ID","WY","PA","RI","OR","OH","IA","NJ","WV","MA","MO","OK","IN","VA","CO","WA","AZ","NV","AK","NY","TX","MS","KY","AR","TN","CA","NM","DE")
cluster2 <- c("NC","GA","AL","FL","MD","SC","LA","DC")

XS["Cluster"]=rep(1,1996)
XS$Cluster[XS$state %in% cluster2] <- 2

plot_usmap(labels = T,
    data = XS[c("state","Cluster")], values = "Cluster",  color = "black") + 
    scale_fill_gradient(low = Col[2], high =  Col[1]) + 
  theme(legend.position = "none")+
    labs(title = "2 clusters by ViolentCrimesPerPop Mean similarity")
```
The linear model considering a new categorical predictor, that factorize the cluster to which each state belongs, is then built. The cluster information is now significative, moreover the results on the test set show a decrease of the MSE and equal adjusted $R^2$ (compared to the equivalent model without categorical predictor).

```{r}
XS = XS[,-53]
XS$Cluster <- as.factor(XS$Cluster)

is.factor(XS$Cluster) 

XS.train <- subset(XS, (row.names(XS) %in% train.sample))
XS.test <- subset(XS, (row.names(XS) %in% test.sample))


regc.out <- lm(Y.train~. , data=XS.train) #application of LR with a categorical variable
summary(regc.out)
```


```{r}
# prediction 
pred = predict(regc.out, XS.test)

n <- 400
p <- 53

rss <- sum((Y.test - pred)^2)       # Residual Sum of Squares
ess <- sum((pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

Finally ANCOVA is performed, to asses if the two models differ between the two groups. The previous summary proves that the intercepts are different, now we want to test if also some of the slope coefficients vary between the two clusters.
To this end the linear model exploiting also interaction effects between the Cluster variable and all the other is built.

```{r}
regc.out <- lm(Y.train~.*Cluster , data=XS.train) #add interaction and main effect due to Cluster
summary(regc.out)
```

```{r}
# prediction 
pred = predict(regc.out, XS.test)

n <- 400
p <- 105

rss <- sum((Y.test - pred)^2)       # Residual Sum of Squares
ess <- sum((pred - mean(Y.test))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n                               #Mean Squared Error
cat("MSE: ", round(mse,2), "\n")
cat("adjR^2: ", round(adjr2,2), "\n")
```

The summary shows that there are significative slope changes for coefficients like agePct12t21, blackPerCap, pctWFarmSelf, PctEmploy, MalePctNevMarr and MedRentPctHousInc. Note that many of these are right among the more influential predictors emerged from the feature selection section.
However this more complex model does not perform better than others in terms of MSE, as a matter of fact the adjusted $R^2$ is considerably smaller.

In conclusion, the contribution of the variable State, although it seems to have influences on the response, it does not seem to bring many improvements to the simplified model tested so far. Moreover not having considered it does not therefore seem to be problematic, making us believe that the model adopted is a good compromise between valid representation of data and simplicity.

## Multiple Responses Linear Regression

We conclude the experimentation of continuous regression models testing the contemporary prediction of all the crime rates related response variables: murdPerPop, rapesPerPop, robbbPerPop, assaultPerPop, burglPerPop, larcPerPop, autoTheftPerPop, arsonsPerPop, ViolentCrimesPerPop and nonViolPerPop (with reasonings similar to those made above, logarithmic transformation was used for each of them).
Note that this variables are correlated between them, and hence it is reasonably  to expect that they depend on the same predictors, this is the main motivation to build such model, moreover training the regressor considering all of them could have benefit effect. 
As a matter of fact, the MSE results on the test are good for almost all the responses.

```{r}
#predict all the crime features per pop 
X = subset(corrdf, select = - c(ViolentCrimesPerPop))
X.train <- subset(X, (row.names(X) %in% train.sample))
X.test <- subset(X, (row.names(X) %in% test.sample))


Ys<-cbind(log(standf["murdPerPop"]+1), log(standf["rapesPerPop"]+2), log(standf["robbbPerPop"]+1), log(standf["assaultPerPop"]+1), log(standf["burglPerPop"]+2), log(standf["larcPerPop"]+2), log(standf["autoTheftPerPop"]+1), log(standf["arsonsPerPop"]+1), log(standf["ViolentCrimesPerPop"]+1), log(standf["nonViolPerPop"]+2))

Ys.train<- as.matrix(subset(Ys, (row.names(Ys) %in% train.sample)))
Ys.test<- as.matrix(subset(Ys, (row.names(Ys) %in% test.sample)))

mm <- lm( Ys.train ~., data=X.train)
#summary(mm) #long output
```


```{r}
# predictions 
n <- 400
p <- 52

pred = predict(mm, X.test)
rss <- colSums((as.data.frame(Ys.test) - as.data.frame(pred))^2)       # Residual Sum of Squares
ess <- colSums((pred - mean(as.matrix(Ys.test)))^2) # Explained Sum of Squares
tss <- ess + rss                           # Total Sum of Squares
r2 <- 1 - rss/tss                          # R Squared statistic
adjr2 <- 1 - (1-r2)*((n-1)/(n-p-1))        # adjusted R square 

mse <- rss/n

as.data.frame(cbind(mse,adjr2))
```



## Binary classification models

The last section of our research is dedicated to binary classification. The capability of logistic regressor and KNN algorithm have been tested in the classification task among two level of ViolentCrimesPerPop: High and Low. 
In particular, we set a threshold in order to distinguish if a community has high level of crimes per population. Different options were possible for the value of this threshold such as considering the mean among all the US provided in [see refer], the mean or the median of our entire dataset.  

For convenience in this simplistic study, we decided to proceed using the median of ViolentCrimesPerPop, the main reasons of this choice are that, first of all, it is more representative than the mean since the distribution is skewed, secondly this value split the data in balanced classes.

```{r}

#HLthreshold <- 0 # threshold is the mean (0 since standardized)
HLthreshold <- median(standf$ViolentCrimesPerPop) # threshold is the median (balanced classes)

YF.train <- Y.train
YF.train[Y.train > HLthreshold] <- "High"
YF.train[!Y.train > HLthreshold]<- "Low"
YF.train <- as.factor(YF.train)

YF.test <- Y.test
YF.test[Y.test > HLthreshold] <- "High"
YF.test[!Y.test > HLthreshold]<- "Low"
YF.test <- as.factor(YF.test)

```

### Logistic regression

The logistic regression is built below.
```{r}
lreg.out<-glm(YF.train~., family = binomial, data=X.train)

# check the coding of ViolentCrimesFactor
contrasts(YF.train)

logistic.prob <- predict(lreg.out, type="response") # want probability
```

In order to select the best threshold for the probability, the ROC curve is plotted. The shape of the function and the AUC value confirm the acceptable quality of the model.
```{r}
# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(YF.train, logistic.prob, levels=c("Low", "High"), transpose = TRUE)

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
```

```{r}
logistic.pred <- rep("High", 1596)
logistic.pred[logistic.prob>0.545] <- "Low"

#train accuracy
table(logistic.pred,YF.train)
mean(logistic.pred==YF.train)

```

```{r}
#test accuracy
logistic.prob <- predict(lreg.out, X.test, type="response") # want probability
logistic.pred <- rep("High", 400)
logistic.pred[logistic.prob>0.545] <- "Low"

table(logistic.pred,YF.test)
mean(logistic.pred==YF.test)
```

With this choice of the probability threshold, the model achieves good sensitivity and specificity and an accuracy on the test set of 85%.

### KNN

Finally we improved a completely non-parametric and non-linear approach: KNN. In this method no assumptions are made about the shape of the decision boundary, moreover differently from LDA or QDA, normality condition of the predictors are not necessary.

Strictly speaking, we first performed 10 folds cross validation to choose the best value for k, i.e. the number of closest training observations to consider to assign the majority class to a new point.

```{r}
#10 fold cross validation
XY.train=as.data.frame(cbind(X.train,YF.train))
XY.train$YF.train<- as.factor(XY.train$YF.train)
is.factor(XY.train$YF.train)

trControl <- trainControl(method  = "cv",number  = 10)

fit <- train(YF.train ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trControl,
             metric     = "Accuracy",
             data       = XY.train)

fit

```

From the C.V., the best k is 5, then the model is performed with this value on the test set, returning an accuracy of 81.5%.
Note that the performance is slightly poorer than the linear, confirming us the power of this simple and fully interpretable model even with a large dataset, with many possible correlated features, as the one under investigation.

```{r}
# test accuracy
knn.pred <- knn(X.train, X.test, YF.train, k=5)
table(knn.pred,YF.test)
mean(knn.pred==YF.test)
```


# Conclusions - Comparing results
Siamo stati bravi Rover, guarda che bei grafici.. (oh no)

#References

- An Introduction to Statistical Learning
- https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized
- https://online.stat.psu.edu/stat462/node/171/
- http://www.disastercenter.com/crime/uscrime.htm



