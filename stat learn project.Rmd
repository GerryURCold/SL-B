---
title: "Statistical Learning Project"
author: "Caria Natascia, Cozzolino Claudia and Petrella Alfredo"
date: "26 maggio 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

# Aim of the project

# Dataset Description

# Exploratory Data Analysis

We know from documentation that missing values are stored as "?".

```{r}
# import dataset
crimedata <- read.csv("crimedata.csv", na.strings="?")
#View(crimedata)
dim(crimedata)
```
2215 rows and 147 columns

Attributes information:

* 4 non-predictive (communityname, state, countyCode, communityCode, fold)
* 125 predictive
* 18 potential predictor (??? intendete response?) (murders, murdPerPop, rapes, rapesPerPop, robberies, robbbPerPop, assaults, assaultPerPop, burglaries,
  burglPerPop, larcenies, larcPerPop, autoTheft, autoTheftPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop)

From UCI description (see https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized) we know that the variables communityname and state are nominal while the remaining are all numeric.
```{r}
# check if variables communityname and state are stored as factors
is.factor(crimedata$communityname)
is.factor(crimedata$state)
```

```{r}
# check number of numeric variables: 145 (= 147 - 2) expected
sum(sapply(crimedata, is.numeric))
```

## Missing values

```{r}
# handle missing values

# inspect total NA
sum(is.na(crimedata))
mean(is.na(crimedata))
```
More than 10% of values is missing.

```{r}
# install.packages("varhandle", dependencies = T)
library(varhandle)
nadf <- inspect.na(crimedata, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=TRUE, na.value = NA)

# nadf dataframe with columns that present NA 
#nadf
```
There are 41 columns with missing values, many of them with more than 50% of the data missing.
Among them:

* 3 non-predictive (countyCode, communityCode, fold)
* Many "Lemas" ---> Check info on these variables to understand what they refer
* Policing information
* Potential Predictor (??? intendete response?) (rapes, rapesPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop in particular)

From documentaion:

The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the 
United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of
rapes. These resulted in missing values for rape, which resulted in missing values for per capita violent crime. Many of these omitted
communities were from the midwestern USA (Minnesota, Illinois, and Michigan have many of these).

The per capita nonviolent crime variable was calculated using the sum of crime variables considered non-violent crimes in the United
States: burglaries, larcenies, auto thefts and arsons. (There are many other types of crimes, these only include FBI 'Index Crimes').

Looking to the dataset, other suspecious values equal to 0.00 have been found
```{r}
zero_count <- function(x) sum(x==0, na.rm = TRUE)
zerodf <- lapply(crimedata, zero_count)[lapply(crimedata, zero_count)!=0]
names(zerodf)
```
Suspecious zeros: numbUrban, pctUrban.

The columns with more than 50% NA are dropped and also variables fold (no useful), OwnOccQrange, RentQrange (obtained as the difference
of other columns). We decide to keep the other columns with zero values.
```{r}
# Find columns with > 50% NA
coltodrop <- as.vector(nadf$column_name[nadf$ratio_of_NA>0.50])

#add other columns to remove
coltodrop <- c(coltodrop, "fold", "OwnOccQrange", "RentQrange")

#add variable that are statistical measures of others (quartiles) 
coltodrop <- c(coltodrop, "OwnOccLowQuart","OwnOccHiQuart", "RentLowQ", "RentHighQ")
coltodrop
```

### Possible ways to handle missing values:
Firstly, all columns with > 50% NA are removed.
```{r}
# Drop columns with > 50% NA
cleandf <- crimedata[,!(names(crimedata) %in% coltodrop)]
#View(cleandf)
```

Then, the remaining variables with missing values are the ones related to the crimes and another one, the variable OtherPerCap (per
capita income for people with 'other' heritage), which has only one missing value.
```{r}
# Remaining columns with NA
nadf <- inspect.na(cleandf, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=FALSE, na.value = NA)
nacolnames <- as.vector(nadf$column_name)
nacolnames
```

Possible ways to handle the remaining missing values:
1. drop all raws with at least one missing value;
2. substitute a missing value with the average computed over the state a community belongs;
3. leave NA value and consider it as another category.

We decide to proceed as in point 2.
```{r}
# dataframe of the columns which still contain NA with the mean computed over the state
meandf <- aggregate(cleandf[,nacolnames], list(cleandf$state), function(x) mean(x, na.rm = TRUE))
sum(is.na(meandf))
```
The dataframe with the mean contains NaN values, this means that there are States for which the value of a certain feature is zero for
all communities that belong to them.
This happens for IL, MI (no data of rapes, rapesPerPop, ViolentCrimesPerPop), KS, VT (no data of arsons, arsonsPerPop, nonViolPerPop).
Moreover from documentation we know that also MN has a lot of missing values for rapes (59 out of 66):
```{r}
mnNA = sum(is.na(cleandf[cleandf$state=="MN","rapes"]))
mnTot = length(cleandf[cleandf$state=="MN","rapes"])
c(mnNA, mnTot, mnNA/mnTot)
```

Therefore, data related to those states is removed.
```{r}
rowtodrop <- as.numeric(rownames(cleandf[cleandf$state %in% c("IL","MI","MN","KS","VT"),]))

cleandf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]
```

For the remaining variables with missing values, we substitute NA with the mean computed over the state
```{r}
# Substitute a missing value with the average computed over the state
for(col in nacolnames) {
  match_table <- tapply(cleandf[[col]], cleandf$state, mean, na.rm=TRUE)
  NA_position <- which(is.na(cleandf[[col]]))
  cleandf[[col]][NA_position] <- match_table[cleandf$state[NA_position]]
}
```

Now cleandf doeas not contain NA
```{r}
sum(is.na(cleandf))
```

```{r}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf", "crimedata")))
```

### Correlation insight

```{r}
# restrict to numeric independent attributes
corrdf <- cleandf[1:98]
corrdf <- corrdf[sapply(corrdf,is.numeric)]

# correlation matrix
cm <- cor(corrdf, use = 'complete.obs')
# only strongly correlated attributes highlighted
cma <- abs(cm) > 0.9
# number of strong correlations
(sum(cma)-dim(corrdf)[2])/2

# correlation matrix and filter plot
# install.packages('corrplot')
library(corrplot)
corrplot(cm,title='Correlation matrix',method='shade',type='lower',tl.pos='l',tl.cex=0.35) # order=!!!
corrplot(cma,title='Correlation matrix',method='shade',type='lower',tl.pos='l',tl.cex=0.35)

# ??? correlated couples
corrcol <- c()
for(i in 2:dim(cm)[1]) {
  for(j in 1:(i-1)) {
    if(cma[i,j]) {
      corrcol = c(corrcol,c(i,j))
    }
  }
}
corrcolname <- matrix(lapply(corrcol,function(x) colnames(corrdf)[x]),nrow=2)
corrcolname

# ??? ignore it right now, could help in the future

rem9 = c(1,9,11,17,20,28,61,65,88,92,93,94) #in caso da rifare perche' tolte colonnee :(

corrdf_ind <- corrdf[-rem9]
# View(corrdfind)

# final correlation matrix and filter
cm <- cor(corrdf_ind, use='complete.obs')
cma <- abs(cm) > 0.9

# perCapInc vs whitePerCap

corrplot(cma,title='Correlation matrix',method='shade',type='lower',tl.pos='l',tl.cex=0.35)




# altri comandi per trovare high correlated... (aiuto non volevo mettermi alla tv :3)
corrdf <- cleandf[1:98]
corrdf <- corrdf[sapply(corrdf,is.numeric)]
rs <- cor(corrdf, use = "complete.obs")
diag(rs) <- NA
col_has_over_90 <- apply(rs, 2, function(x) any(x > .9, na.rm = TRUE))

colnames(rs[, col_has_over_90])
dim(rs[, col_has_over_90])

#selezionate a mano e interpretando significato
coltodropcorr<-c("agePct16t24","numbUrban","medFamInc","perCapInc","PctNotHSGrad","PctBSorMore","MalePctDivorce","FemalePctDiv","NumInShelters","MedRent","PctPersOwnOccup","PctHousOwnOcc","PctKids2Par","PctYoungKids2Par","PctImmigRec8","PctRecImmig8","PctImmigRecent", "PctRecentImmig")
cleandfcorr <- cleandf[,!(names(cleandf) %in% coltodropcorr)]

```

### Standardization / Normalization

```{r}
# Standardization using scale
scaledf <- cleandf
scaledf[seq(3,dim(scaledf)[2])] <- scale(cleandf[seq(3,dim(cleandf)[2])] )
```

```{r}
# Standardization using an home made function (⌐■_■)
standardization <- function(x) {
return ((x - mean(x)) / sd(x))
}

standf <- cleandf
standf[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], standardization)
```

```{r}
# Normalization using an home made function (⌐■_■)
normalization <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

normdf <- cleandf
normdf[seq(3,dim(normdf)[2])] <- lapply(cleandf[seq(3,dim(cleandf)[2])], normalization)
```

```{r, eval=FALSE, include=FALSE}
scaleddf <- data.frame(cleandf2)
scaleddf['population'] <- scale(cleandf['population'], center = rep(1/2))
# scale fa questo (con center=F): col/sqrt(sum(col^2)/(length(col)-1))
# se vogliamo la solita conviene scriverla a mano
```

### Outliers

```{r}
# Remove row if one of its value is
# 3 times greater than the upper interquartile bound or
# 3 times lower than the lower interquartile bound

is.out.IQR <- function(x){
  Q <- quantile(x, probs=c(.25, .75))
  iqr <- IQR(x)
  up <-  Q[2]+3*iqr # Upper bound  
  low <- Q[1]-3*iqr # Lower bound
  out <- (x < low | x > up)
  return(out)
}

is.out.sd <- function(x){
  sd <- sd(x)
  up <-  mean(x)+3*sd # Upper bound  
  low <- mean(x)-3*sd # Lower bound
  out <- (x < low | x > up)
  return(out)
}

outdf <- cleandf
outdf[seq(3,dim(outdf)[2])] <- lapply(cleandf[seq(3,dim(cleandf)[2])], is.out.sd)

odf <- inspect.na(outdf, hist = FALSE, byrow=TRUE, barplot = FALSE, na.value = TRUE)
rowtodrop <- odf$row_name

newdf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]



outdf <- cleandfcorr
outdf[seq(3,dim(outdf)[2])] <- lapply(cleandfcorr[seq(3,dim(cleandfcorr)[2])], is.out.sd)

odf <- inspect.na(outdf, hist = FALSE, byrow=TRUE, barplot = FALSE, na.value = TRUE)
rowtodrop <- odf$row_name

newdfcorr <- cleandfcorr[!(rownames(cleandfcorr) %in% rowtodrop),]

```

```{r}
# Normalization of the dataset without drastic outliers

normalization <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

normnewdf <- newdf
normnewdf[seq(3,dim(normnewdf)[2])] <- lapply(newdf[seq(3,dim(newdf)[2])], normalization)


normnewdfcorr <- newdfcorr
normnewdfcorr[seq(3,dim(normnewdfcorr)[2])] <- lapply(newdfcorr[seq(3,dim(newdfcorr)[2])], normalization)
```

```{r}
# Remove all variables from environment excpet ...
rm(list=setdiff(ls(), c("crimedata", "cleandf", "scaledf", "standf", "normdf", "newdf", "normnewdf","cleandfcorr","newdfcorr","normnewdfcorr")))
```

## Principal Component Analysis

```{r}
# choose the version of the df you prefer (cleandf, normnewdf, scalednewdf ???)
PCAdf <- normnewdf[,3:(dim(normnewdf)[2]-18)]

# PCA computation
pc <- princomp(PCAdf, cor=TRUE) # cor=TRUE to obtain it from the correlation matrix (use it for the unnormalized dataframe)

str(pc)
# first k principal components
k <- 40
plot(pc, npcs=k, cex.names=0.5,las=2, col="#00AFBB", main='Principal Components')

barplot(pc$loadings[,1], cex.names=0.5,las=2, col=  "#FC4E07")

infl1 <- colnames(PCAdf)[abs(pc$loadings[,1])>0.15]
infl1

#install.packages('factoextra')
library(factoextra)
fviz_pca_var(pc,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T,     # Avoid text overlapping
             select.var = list(contrib = 15)
             )




PCAdfcorr <- normnewdfcorr[,3:(dim(normnewdfcorr)[2]-18)]

# PCA computation
pccorr <- princomp(PCAdfcorr, cor=TRUE) # cor=TRUE to obtain it from the correlation matrix (use it for the unnormalized dataframe)

str(pccorr)
# first k principal components
k <- 40
plot(pccorr, npcs=k, cex.names=0.5,las=2, col="#00AFBB", main='Principal Components')

barplot(pccorr$loadings[,1], cex.names=0.5,las=2, col=  "#FC4E07")

infl1corr <- colnames(PCAdfcorr)[abs(pccorr$loadings[,1])>0.15]
infl1corr

#install.packages('factoextra')
library(factoextra)
fviz_pca_var(pccorr,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T,     # Avoid text overlapping
             select.var = list(contrib = 15)
             )


```


## Singular Value Decomposition

An alternative way to reduce the dimensionality of the dataset is to compute the Singular Value Decomposition on the matrix $X$ associated to the model, obtaining two unitary matrices $U$ and $V$ and a rectangular diagonal matrix of singular values $S$ such that $X = USV^T$, where the values on the diagonal of $S$ are sorted in descendent order and the first $k$ columns of $V$ select the correspondent first $k$ most informative combinantions of the attributes of the original dataset, i.e. if we call $V_k$ the matrix containing the first $k$ columns of $V$, $X_k=XV_k$ is a transformation of our dataset with just $k$ columns that contains the most part of information of $X$.

```{r}
# choose the version of the df you prefer (scaledf, normdf)
SVDdf <- normnewdf[,3:102]

# SVD on the correlation matrix
sv <- svd(SVDdf)

# plot the singolar values
barplot(sv$d)

# SVD reduced model
k <- 20
V <- sv$v
matrix(SVDdf)

SVDreddf <- data.matrix(SVDdf) %*% V[,1:k]

#C.sv??
# check that U and V are the same in this case
#sum(!C.sv$u-C.sv$v<0.000001)
```

```{r, eval=FALSE, include=FALSE}
C <- cov(SVDdf)

# plot the original singular values (our actual diagonal is squared)
#barplot(sqrt(C.sv$d)*(dim(SVDdf)[1]-1))
barplot(sv$d)

sv$d
#sqrt(C.sv$d)*(dim(SVDdf)[1]-1)

#???
```


## State Aggregation - data analysis

```{r}
# Violent Crimes per 100K Population - State aggregation
vc_state <- aggregate(crimedata[, "ViolentCrimesPerPop"], list(crimedata$state), mean)
names(vc_state)[names(vc_state) == "Group.1"] <- "state"
names(vc_state)[names(vc_state) == "x"] <- "ViolentCrimesMean"
```

```{r}
# Population - State aggregation
pop_state <- aggregate(crimedata[, "population"], list(crimedata$state), sum)
names(pop_state)[names(pop_state) == "Group.1"] <- "state"
names(pop_state)[names(pop_state) == "x"] <- "PopTot"
```

```{r, include=FALSE}
# install.packages("usmap")
library(usmap)
library(ggplot2)
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = vc_state, values = "ViolentCrimesMean", color = "red") + 
  scale_fill_continuous(low = "white", high= "red", name = "Violent Crimes", label = scales::comma) + 
  theme(legend.position = "right")
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = pop_state, values = "PopTot", color = "red") + 
  scale_fill_continuous(low = "white", high= "red", name = "Population", label = scales::comma) + 
  theme(legend.position = "right")
```
Montana, Nebraska and Hawaii are missing


Maps plot on mean values per state
```{r}
#plot mean some predictor and all response vars per state

#data=cleandf
dataf=newdfcorr


for(i in c(3,13,26,29,44, seq(dim(dataf)[2]-16,dim(dataf)[2]-2, by=2),dim(dataf)[2]-1,dim(dataf)[2])) {
  dataaggr <- aggregate(dataf[, colnames(dataf)[i]], list(dataf$state), mean)
  names(dataaggr)[names(dataaggr) == "Group.1"] <- "state"
  names(dataaggr)[names(dataaggr) == "x"] <- colnames(dataf)[i]
  
  mycol=c("#00AFBB", "#E7B800", "#FC4E07")[i%%3 + 1]


  pl<-plot_usmap(data = dataaggr, values = colnames(dataf)[i], color = 1) + 
    scale_fill_continuous(low = "white", high= mycol, name = colnames(dataf)[i], label = scales::comma) +     theme(legend.position = "right")
  
  print(pl)

}



```

## Pairs plot

```{r}
# pairs plot prova
pairs(~ medIncome+racepctblack+racePctAsian+racePctWhite+ViolentCrimesPerPop,data=cleandf)
```

```{r}
#scatter plot between all predictor vs ViolentCrimesPerPop (tantii)
par(mfrow=c(2,2))

#data=cleandf
dataf=normnewdfcorr

#seq(3,dim(dataf)[2]-18)

#for(i in seq(3,dim(dataf)[2]-18)) {
  
  #scatter.smooth(dataf[,i],dataf$ViolentCrimesPerPop, xlab=colnames(dataf[i]), ylab='ViolentCrimesPerPop')

#}

par(mfrow=c(1,1))

```


```{r}
boxplot(dataf$PctPopUnderPov~dataf$state, cex.names=0.1,las=2,col=c("#00AFBB", "#E7B800", "#FC4E07"))
``` 

```{r}
#non hanno senso :(
#hist(dataf$PctPopUnderPov)
``` 
## Models

### Linear regression
```{r}
#use normalized-outliers 'free' data
m0<-lm( normnewdf$ViolentCrimesPerPop~. , data=normnewdf[,3:98])
summary(m0)

#use corresponding pc 
pc0 <- lm(normnewdf$ViolentCrimesPerPop~., data=as.data.frame(pc$scores))
summary(pc0)

#use only first 15 comp
pc01 <- lm(normnewdf$ViolentCrimesPerPop~., data=as.data.frame(pc$scores[,1:15]))
summary(pc01)

#use normalized-outliers and high correlated 'free' data
m1<-lm( normnewdfcorr$ViolentCrimesPerPop~. , data=normnewdfcorr[,3:80])
summary(m1)

#use corresponding pc 
pc1 <- lm(normnewdfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores))
summary(pc1)

#use only first 10 comp
pc11 <- lm(normnewdfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores[,1:15]))
summary(pc11)

#use only comp with '***' significativity
pc12 <- lm(normnewdfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)]))
summary(pc12)

plot(pc12)



scatter.smooth(pccorr$scores[,8],normnewdfcorr$ViolentCrimesPerPop)
```


### Multivariate Linear Regression
```{r}
#predict all the crime features... 
m2 <- lm(data.matrix(normnewdfcorr[,81:98]) ~. , data=as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)]))

#summary(m2) #long output

```

### LDA e QDA?

### Bayesian regression?
Issue... non abbiamo possibili valori ipotesi iniziali ...

### Binary classification models and KNN
pensando di trasformare le ultime colonne in qualcosa del tipo 'HIGH'/'LOW'... ha senso? 



## eventually useful stuff

* raceP(p)ct(...) columns sum up to more than 1, one hot encoding???
* numbUrban or pctUrban keep only one (maybe the second, so that we can eventually discard population)
  same for  NumUnderPov & PctPopUnderPov
            NumKidsBornNeverMar & PctKidsBornNeverMar (total missing)
            HousVacant & PctHousOccup & PctHousOwnOccup
            OwnOccLowQuart & OwnOccMedVal & OwnOccHiQuart & OwnOccQrange
            RentQrange (difference between 2 prev cols)
            Lemas(...) ALREADY REMOVED in cleandf
            PopDens (again, delete pop or similar)
            
            ?Target? & ?Target?PerPoP
* medIncome and medfamincome so similar
* PctImmigRec(...): compress in fewer columns with the coefficients describing the evolution ??? (obviously increasing (inclusion): at what rate?)
  same for PctBornSameState & PctSame(...)
* PctRec(...): same but over the full population
