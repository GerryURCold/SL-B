---
title: "Statistical Learning Project"
author: "Caria Natascia, Cozzolino Claudia and Petrella Alfredo"
date: "26 maggio 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

```{r , include=FALSE}
library(car)

library(ggplot2)

# install.packages("usmap")

library(usmap)

# install.packages("varhandle", dependencies = T)
library(varhandle)

library(pROC)

library(MASS)

library(class)

# install.packages('corrplot')
library(corrplot)

# install.packages('factoextra')
library(factoextra)


set.seed(1)
```

# Aim of the project

# Dataset Description

# Data Cleaning

We know from documentation that missing values are stored as "?".

```{r}
# import dataset
crimedata <- read.csv("crimedata.csv", na.strings="?")
dim(crimedata)
```
2215 rows and 147 columns

Attributes information:

* 4 non-predictive (communityname, state, countyCode, communityCode, fold)
* 125 predictive
* 18 potential predictor (??? intendete response?) (murders, murdPerPop, rapes, rapesPerPop, robberies, robbbPerPop, assaults, assaultPerPop, burglaries,
  burglPerPop, larcenies, larcPerPop, autoTheft, autoTheftPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop)

From UCI description (see ref sito web [num bibliografia]) we know that the variables communityname and state are nominal while the remaining are all numeric.
```{r}
# check if variables communityname and state are stored as factors
is.factor(crimedata$communityname)
is.factor(crimedata$state)
```

```{r}
# check number of numeric variables: 145 (= 147 - 2) expected
sum(sapply(crimedata, is.numeric))
```

## Missing values

```{r}
# handle missing values

# inspect total NA
sum(is.na(crimedata))
mean(is.na(crimedata))
```
More than 10% of values is missing.

```{r}
# nadf dataframe with columns that present NA 
nadf <- inspect.na(crimedata, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=TRUE, na.value = NA)
```
There are 41 columns with missing values, many of them with more than 50% of the data missing.
Among them:

* 3 non-predictive (countyCode, communityCode, fold)
* Many "Lemas" ---> Check info on these variables to understand what they refer
* Policing information
* Potential Predictor (??? intendete response?) (rapes, rapesPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop in particular)

From documentaion:

The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the 
United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of
rapes. These resulted in missing values for rape, which resulted in missing values for per capita violent crime. Many of these omitted
communities were from the midwestern USA (Minnesota, Illinois, and Michigan have many of these).

The per capita nonviolent crime variable was calculated using the sum of crime variables considered non-violent crimes in the United
States: burglaries, larcenies, auto thefts and arsons. (There are many other types of crimes, these only include FBI 'Index Crimes').

Looking to the dataset, other suspecious values equal to 0.00 have been found
```{r}
zero_count <- function(x) sum(x==0, na.rm = TRUE)
zerodf <- lapply(crimedata, zero_count)[lapply(crimedata, zero_count)!=0]
names(zerodf)
```
Suspecious zeros: numbUrban, pctUrban.

The columns with more than 50% NA are dropped and so were the variables fold (not useful), OwnOccQrange and RentQrange (obtained as the difference
of other columns). We decided to keep the other columns with zero values due to their meaningfulness.
```{r}
# Find columns with > 50% NA
coltodrop <- as.vector(nadf$column_name[nadf$ratio_of_NA>0.50])

# add other columns to remove
coltodrop <- c(coltodrop, "fold", "OwnOccQrange", "RentQrange")
coltodrop
```

### Possible ways to handle missing values:
Firstly, all columns with > 50% NA are removed.
```{r}
# Drop columns with > 50% NA and clearly redundant ones
cleandf <- crimedata[,!(names(crimedata) %in% coltodrop)]

```

Then, the remaining variables with missing values are the ones related to the crimes and another one, the variable OtherPerCap (per
capita income for people with 'other' heritage), which has only one missing value.
```{r}
# Remaining columns with NA
nadf <- inspect.na(cleandf, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=FALSE, na.value = NA)
nacolnames <- as.vector(nadf$column_name)
nacolnames
```

Possible ways to handle the remaining missing values:
1. drop all raws with at least one missing value;
2. substitute a missing value with the average computed over the state a community belongs;
3. leave NA value and consider it as another category.

We decide to proceed as in point 2.
```{r}
# dataframe of the columns which still contain NA with the mean computed over the state
meandf <- aggregate(cleandf[,nacolnames], list(cleandf$state), function(x) mean(x, na.rm = TRUE))
sum(is.na(meandf))
```

The dataframe with the mean contains NaN values, this means that there are States for which the value of a certain feature is zero for
all communities that belong to them.
This happens for IL, MI (no data of rapes, rapesPerPop, ViolentCrimesPerPop), KS, VT (no data of arsons, arsonsPerPop, nonViolPerPop).
Moreover from documentation we know that also MN has a lot of missing values for rapes (59 out of 66):
```{r}
mnNA = sum(is.na(cleandf[cleandf$state=="MN","rapes"]))
mnTot = length(cleandf[cleandf$state=="MN","rapes"])
c(mnNA, mnTot, mnNA/mnTot)
```

Therefore, data related to those states is removed.
```{r}
rowtodrop <- as.numeric(rownames(cleandf[cleandf$state %in% c("IL","MI","MN","KS","VT"),]))

cleandf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]
```

For the remaining variables with missing values, we substitute NA with the mean computed over the state
```{r}
# Substitute a missing value with the average computed over the state
for(col in nacolnames) {
  match_table <- tapply(cleandf[[col]], cleandf$state, mean, na.rm=TRUE)
  NA_position <- which(is.na(cleandf[[col]]))
  cleandf[[col]][NA_position] <- match_table[cleandf$state[NA_position]]
}
```

Now cleandf doeas not contain NA
```{r}
sum(is.na(cleandf))
dim(cleandf)
```

```{r , include=FALSE}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf", "crimedata")))
```


# Standardization

```{r}
# Standardization using an home made function 
standardization <- function(x) {
return ((x - mean(x)) / sd(x))
}

standf <- cleandf
standf[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], standardization)
```

# Exploratory Data Analysis 
```{r, include=FALSE}
Col = c("#EF476F", "#FFD166", "#06D6A0", "#118AB2", "#073B4C")
Rgb = c(rgb(0.94, 0.28, 0.44, 0.7),
        rgb(1, 0.82, 0.4, 0.7),
        rgb(0.02, 0.84, 0.63, 0.7),
        rgb(0.07, 0.54, 0.7, 0.7),
        rgb(0.03, 0.23, 0.3, 0.7))
```

```{r}
boxplot(cleandf$ViolentCrimesPerPop~cleandf$state, col= Col)
boxplot(standf$ViolentCrimesPerPop~standf$state, col= Col)
boxplot(log(standf$ViolentCrimesPerPop+1)~standf$state, col= Col)
```

```{r}
# Violent Crimes per 100K Population - State aggregation - Mean
vc_state <- aggregate(cleandf[, "ViolentCrimesPerPop"], list(cleandf$state), mean)
names(vc_state)[names(vc_state) == "Group.1"] <- "state"
names(vc_state)[names(vc_state) == "x"] <- "ViolentCrimesMean"
```

```{r}
summary(vc_state$ViolentCrimesMean)
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = vc_state, values = "ViolentCrimesMean", color = "black") +
   scale_fill_continuous(trans="sqrt", low = Rgb[3], high = Rgb[1], name = "Violent Crimes",limits=c(85,3049),
label=scales::comma)+
  theme(legend.position = "right")

```


```{r}
par(mfrow=c(1,2))
hist(cleandf$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes")
lines(density(cleandf$ViolentCrimesPerPop))

hist(log(cleandf$ViolentCrimesPerPop +1), prob=TRUE, breaks=30, col=Col[1], main="log(Violent Crimes +1)")
lines(density(log(cleandf$ViolentCrimesPerPop +1)))
par(mfrow=c(1,1))


par(mfrow=c(1,2))
hist(standf$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes")
lines(density(standf$ViolentCrimesPerPop))

hist(log(standf$ViolentCrimesPerPop +1), prob=TRUE, breaks=30, col=Col[1], main="log(Violent Crimes +1)")
lines(density(log(standf$ViolentCrimesPerPop +1)))
par(mfrow=c(1,1))

```

```{r}
par(mfrow=c(1,2))
qqnorm(cleandf$ViolentCrimesPerPop, main="Violent Crimes")
qqline(cleandf$ViolentCrimesPerPop)

qqnorm(log(cleandf$ViolentCrimesPerPop +1), main="log(Violent Crimes +1)")
qqline(log(cleandf$ViolentCrimesPerPop +1))
par(mfrow=c(1,1))

par(mfrow=c(1,2))
qqnorm(standf$ViolentCrimesPerPop, main="Violent Crimes")
qqline(standf$ViolentCrimesPerPop)

qqnorm(log(standf$ViolentCrimesPerPop +1), main="log(Violent Crimes +1)")
qqline(log(standf$ViolentCrimesPerPop +1))
par(mfrow=c(1,1))

```

```{r}
violent_names <- c("murders", "rapes", "robberies", "assaults")
violPerPop_names <- c("murdPerPop", "rapesPerPop", "robbbPerPop", "assaultPerPop")

violent <- cleandf[, (colnames(cleandf) %in% violent_names)]
violPerPop <- cleandf[, (colnames(cleandf) %in% violPerPop_names)]
```

```{r}
par(mfrow = c(2,2))
boxplot(violent, outline = TRUE, col = Col[seq(1:4)])
boxplot(violent, outline = FALSE, col = Col[seq(1:4)])

boxplot(violPerPop, outline = TRUE, col = Col[seq(1:4)])
boxplot(violPerPop, outline = FALSE, col = Col[seq(1:4)])
par(mfrow=c(1,1))
```

```{r}
# Non Violent Crimes per 100K Population - State aggregation - Mean
nvc_state <- aggregate(cleandf[, "nonViolPerPop"], list(cleandf$state), mean)
names(nvc_state)[names(nvc_state) == "Group.1"] <- "state"
names(nvc_state)[names(nvc_state) == "x"] <- "NonViolentCrimesMean"
```

```{r}
summary(nvc_state$NonViolentCrimesMean)
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = nvc_state, values = "NonViolentCrimesMean", color = "black") +
   scale_fill_continuous(trans="sqrt",low = Rgb[3], high = Rgb[1], name = "Non Violent Crimes",limits=c(2825,9252),
label=scales::comma)+
  theme(legend.position = "right")
```

```{r}
par(mfrow=c(1,2))
hist(cleandf$nonViolPerPop, prob=TRUE, breaks=30, col=Col[2], main="Non Violent Crimes")
lines(density(cleandf$nonViolPerPop))

hist(log(cleandf$nonViolPerPop+1), prob=TRUE, breaks=30, col=Col[2], main="log(Non Violent Crimes +1)")
lines(density(log(cleandf$nonViolPerPop+1)))
par(mfrow=c(1,1))

par(mfrow=c(1,2))
hist(standf$nonViolPerPop, prob=TRUE, breaks=30, col=Col[2], main="Non Violent Crimes")
lines(density(standf$nonViolPerPop))

hist(log(standf$nonViolPerPop+1), prob=TRUE, breaks=30, col=Col[2], main="log(Non Violent Crimes +2)")
lines(density(log(standf$nonViolPerPop+2)))
par(mfrow=c(1,1))
```

```{r}
par(mfrow=c(1,2))
qqnorm(cleandf$nonViolPerPop, main="Non Violent Crimes")
qqline(cleandf$nonViolPerPop)

qqnorm(log(cleandf$nonViolPerPop +1), main="log(Non Violent Crimes +1)")
qqline(log(cleandf$nonViolPerPop +1))
par(mfrow=c(1,1))

par(mfrow=c(1,2))
qqnorm(standf$nonViolPerPop, main="Non Violent Crimes")
qqline(standf$nonViolPerPop)

qqnorm(log(standf$nonViolPerPop +2), main="log(Non Violent Crimes +2)")
qqline(log(standf$nonViolPerPop +2))
par(mfrow=c(1,1))
```

```{r}
nonViol_names <- c("burglaries", "larcenies", "autoTheft", "arsons")
nonViolPerPop_names <- c("burglPerPop", "larcPerPop", "autoTheftPerPop", "arsonsPerPop")

nonViol <- cleandf[, (colnames(cleandf) %in% nonViol_names)]
nonViolPerPop <- cleandf[, (colnames(cleandf) %in% nonViolPerPop_names)]
```

```{r}
par(mfrow = c(2,2))
boxplot(nonViol, outline = TRUE, col = Col[seq(1:4)])
boxplot(nonViol, outline = FALSE, col = Col[seq(1:4)])

boxplot(nonViolPerPop, outline = TRUE, col = Col[seq(1:4)])
boxplot(nonViolPerPop, outline = FALSE, col = Col[seq(1:4)])
par(mfrow=c(1,1))
```

```{r , include=FALSE}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf", "crimedata","standf")))
```



# Multiple linear regression

response = ViolentCrimesPerPop (forcing the assumpition of equal mean between state t)

Remove non predictive variables:
communityname, state, all crimes except variable ViolentCrimesPerPop
```{r}
coltodrop <- c(1,2, seq(103,120)[-17]) # -17 keeps ViolentCrimesPerPop

df <- standf[,-coltodrop]
```

Multiple Linear Regression
```{r}
reg.out <- lm(ViolentCrimesPerPop~., data=df)
```

```{r}
se  <- summary(reg.out)$sigma                  # se
rsq <- summary(reg.out)$r.squared              # R^2
adrsq <- summary(reg.out)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out)
par(mfrow=c(1,1))
```

## Assessing Model Assumptions:
The model assumptions are:

- Linearity of the response-predictor relationships;
- Homeschedasticity: Var($\epsilon_i$) = $\sigma^2$;
- Normality and Independence of $\epsilon_i$.

### Linearity
The plots of residuals versus fitted values shows a little pattern, however it seems to indicate that there are linear associations in
the data. (e ci dice che gli errori sono non correlati :))

### Homoschedasticity
The presence of a funnel shape in the residual plot suggests that the error terms do not have a constant variance. One possible solution
is to transform the response variable $Y$ using a concave function such as $\log(Y)$ or $\sqrt(Y)$ .

```{r}
reg.out2 <- lm(log(ViolentCrimesPerPop+1)~., data=df)
```

```{r}
se  <- summary(reg.out2)$sigma                  # se
rsq <- summary(reg.out2)$r.squared              # R^2
adrsq <- summary(reg.out2)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2)) 
plot(reg.out2)
par(mfrow=c(1,1))
```

The plots of residuals versus fitted values shows that such a transformation leads to a reduction in heteroscedasticity.


### Normality and Independence of errors
We assess normality looking to the QQplot....


## Other Possible Problems

### Outliers
The residual plot identifies some outliers. However, it can be difficult to decide how large a residual needs to be before we consider
the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed
by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute
value are possible outliers.

Note: Why 3? The Studentized Residuals are approximated by a N(0,1). The probability to observe a value greater than 3 is 0.001349898. [ref biblio?]

```{r}
1 - pnorm(3)
```

```{r}
plot(predict(reg.out2), rstandard(reg.out2), xlab="Fitted Values", ylab = "Studentized Residuals")

abline(h=3, col = "red")
abline(h=-3, col = "red")
```
Explore and interpret city of outliers...?

```{r}
out <- names(rstandard(reg.out2)[(abs(rstandard(reg.out2)) > 3)])

standf[out,c(1,2)]
```

```{r}
# regression without outliers
reg.out3 <- lm(log(ViolentCrimesPerPop+1)~., data=df[!(rownames(df) %in% out),])
```

```{r}
se  <- summary(reg.out3)$sigma                  # se
rsq <- summary(reg.out3)$r.squared              # R^2
adrsq <- summary(reg.out3)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2)) 
plot(reg.out3)
par(mfrow=c(1,1))
```

The RSE is 0.56 when the outlier is included in the regression and it is 0.52 when the outlier is removed. Since the RSE is used to
compute all confidence intervals and p-values, this can have implications for the interpretation of the fit. Moreover, dropping the
outliers the $R^2$ increses from 0.67 to 0.7.
Care should be taken in the decision of taking or dropping outliers.

### High Leverage Points
In order to quantify an observations leverage, we compute the leverage statistic. If a given observation has a leverage statistic that
greatly exceeds (p+1)/n, then we may suspect that the corresponding point has high leverage.

```{r}
hv <- hatvalues(reg.out2)
plot(hv, rstandard(reg.out2), xlab="Leverage", ylab = "Studentized Residuals")

p <- dim(df)[2]-1
n <- dim(df)[1]
abline(v=(p+1)/n, col = "red")
```


Try removing high leverage points [ref biblio]
explore city state leverage interpret?

```{r}
lev <- names(hv[hv>3*(p+1)/n])
standf[lev,c(1,2)]
```

```{r}
# regression without leverage points
reg.out4 <- lm(log(ViolentCrimesPerPop+1)~., data=df[!(rownames(df) %in% lev),])

se  <- summary(reg.out4)$sigma                  # se
rsq <- summary(reg.out4)$r.squared              # R^2
adrsq <- summary(reg.out4)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out4)
par(mfrow=c(1,1))
```

Con questo cut non sembra esserci molta differenza... ma RSE diminuisce di qualche millesimo.. e togliamo solo 56 righe... leggo dalla source sopra la seguente...:
a high leverage observation may or may not actually be influential, a data point has large influence only if it affects the estimated regression function.. potrebbe essere una buona notizia :)

Facendo un cut maggiore usando 2*.. l'errore riaumenta



### Collinearity 
Collinearity refers to the situation in which two or more predictor variables
collinearity
are closely related to one another. The presence of collinearity can pose problems in
the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.

Since collinearity reduces the accuracy of the estimates of the regression
coefficients, it causes the standard error for $\hat \beta_j$ to grow. Recall that the
t-statistic for each predictor is calculated by dividing $\hat \beta_j$ by its standard
error. Consequently, collinearity results in a decline in the t-statistic. As a
result, in the presence of collinearity, we may fail to reject $H_0:\ \beta_j = 0$. This
means that the power of the hypothesis test-the probability of correctly
power
detecting a non-zero coefficient-is reduced by collinearity. 

To avoid such a situation, it is desirable to identify and address potential
collinearity problems while fitting the model.
A simple way to detect collinearity is to look at the correlation matrix
of the predictors.

####Correlation insight

In the following few lines of code we perform a first manual skimming of strongly dependent variables removing, between the most
correlated ones, the more redundant and meaningless.

```{r}
# restrict to the predictive numeric attributes
coltodrop <- c(1,2, seq(103,120))
corrdf <- cleandf[,-coltodrop]

# correlation matrix
cm <- cor(corrdf, use='complete.obs')

par(mfrow=c(1,2)) 

# correlation matrix plot
corrplot(cm,title='Initial correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35) # order=!!!

# correlation tradeoff
threshold <- 0.7
# only strongly correlated attributes highlighted
cma <- abs(cm) > threshold
# number of strong correlations
(sum(cma) - dim(corrdf)[2]) / 2

# filtered correlation matrix plot
corrplot(cma,title='Filtered correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

par(mfrow=c(1,1))
```

??? Spiega le scelte e soprattutto quella a 0.7!

```{r}
# columns with correlation with more meaningful ones higher than threshold in absolute value

rem9 <- c( 
"population","agePct16t24","numbUrban","pctWSocSec","medFamInc","perCapInc",        "NumUnderPov","PctLess9thGrade","PctOccupMgmtProf","MalePctDivorce","FemalePctDiv","PctFam2Par",       
"PctKids2Par" ,"NumKidsBornNeverMar", "PctImmigRec5","PctImmigRec10",      
"PctRecImmig5","PctRecImmig8","PctRecImmig10","PctSpeakEnglOnly" ,  
"PctLargHouseOccup","PersPerOccupHous","PctHousOwnOcc","OwnOccLowQuart",     
"OwnOccHiQuart","RentLowQ","RentHighQ","MedRent",            
"NumInShelters","NumStreet","PctForeignBorn")

rem8 <- c(rem9,
"householdsize","racePctWhite","agePct12t29","medIncome",
"pctWWage","pctWPubAsst","PctPopUnderPov","PersPerFam",         
"PctYoungKids2Par","PctWorkMom","PctKidsBornNeverMar", "PctImmigRec8",       
"PctNotSpeakEnglWell", "PctPersDenseHous","OwnOccMedVal","RentMedian",         
"PctSameCity85")

rem7 <- c(rem8,
"pctWInvInc","PctNotHSGrad","PctBSorMore","PctLargHouseFam","PctHousLess3BR", 
"MedNumBR","PctSameState85" )


corrdf_ind <- corrdf[,!(colnames(corrdf) %in% rem7)]

# final correlation matrix and filter
cm_ind <- cor(corrdf_ind, use='complete.obs')
cma_ind <- abs(cm_ind) > threshold

par(mfrow=c(1,1))
# resulting filtered correlation matrix plot
corrplot(cma_ind, title='Filtered correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

# 0.9
# OK perCapInc del. vs whitePerCap (21 del. vs 22, use average)
# OK PctBSorMore kept vs PctOccupMgmtProf (32 kept vs 38, solved by 0.8)
# OK NumKidsBornNeverMar del. vs NumImmig (50 del. vs 52, solved by 0.8)
# 0.8
# OK racepvtblack vs racePctWhite (3 kept vs 4, let's keep minorities)
# OK agePct12t21 kept vs agePct12t29 (7 kept vs 8, solved by 0.7)
# OK medIncome del. vs whitePerCap (13 del. vs 22, use average)
# OK pctWPubAsst del. vs PctUnemployed (18 del. vs 33, solved by 0.7)
# OK PctPopUnderPov del. vs PctHousNoPhone (29 del. vs 78, solved by 0.7)
# OK PctYoungKids2Par del. vs PctTeen2Par (46 del. vs 47, solved by 0.7)
# OK PctWorkMomYoungKids kept vs PctWorkMom (48 kept vs 49, unanimity)
# OK PctImmigRecent kept vs PctImmigRec8 (53 kept vs 55)
# OK PctSameHouse85 kept vs PctSameCity85 (94 kept vs 95, more informative)
# 0.7
# OK NumImmig and HousVacant (52 and 72, unanimity)

# final correlation matrix plot
corrplot(cm_ind, title='Final correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

par(mfrow=c(1,1))
```



When faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression
fit, since the presence of collinearity implies that the information that this
variable provides about the response is redundant in the presence of the
other variables. The second solution is
to combine the collinear variables together into a single predictor.



qui da provare il modello con le colonne tolte dalle analisi sulle collinearity??
```{r}
corrdf <- df[,!(colnames(df) %in% rem7)]

reg.out5 <- lm(log(ViolentCrimesPerPop+1)~., data=corrdf)
#summary(reg.out5)

se  <- summary(reg.out5)$sigma                  # se
rsq <- summary(reg.out5)$r.squared              # R^2
adrsq <- summary(reg.out5)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out5)
par(mfrow=c(1,1))
```

```{r, include=FALSE}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf", "crimedata","standf","df","corrdf")))
```

# Split in Train and Test / validation Folds

```{r}
#train <- sample(rownames(cleandf), 1600) # leave as strings to select original indexes
train <- sample(1:1996, 1600) #piu comodo per ora
```

# Variables selection

## Principal Component Analysis

```{r}
# PCA computation
pc <- princomp(corrdf[,-dim(corrdf)[2]], cor=TRUE) # cor=TRUE to obtain it from the correlation matrix (use it for the unnormalized dataframe)

#str(pc)
# first k principal components
k <- 40
plot(pc, npcs=k, cex.names=0.5,las=2, col="#00AFBB", main='Principal Components')

par(mfrow=c(1,2))
barplot(pc$loadings[,1], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 1')
barplot(pc$loadings[,2], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 2')
barplot(pc$loadings[,3], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 3')
barplot(pc$loadings[,4], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 4')
barplot(pc$loadings[,5], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 5')
barplot(pc$loadings[,6], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 6')
barplot(pc$loadings[,7], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 7')
barplot(pc$loadings[,8], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 8')
barplot(pc$loadings[,9], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 9')
barplot(pc$loadings[,10], cex.names=0.5,las=2, col=  "#FC4E07", main='Component 10')
par(mfrow=c(1,1))

#influent features per component 1 e 2
infl1 <- colnames(corrdf[,-dim(corrdf)[2]])[abs(pc$loadings[,1])>0.15]
infl1
infl2 <- colnames(corrdf[,-dim(corrdf)[2]])[abs(pc$loadings[,2])>0.15]
infl2

fviz_pca_var(pc,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T,     # Avoid text overlapping
             select.var = list(contrib = 15)
             )
```


PC regression

```{r}
reg.pc <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores))
#summary(reg.pc)

se  <- summary(reg.pc)$sigma                  # se
rsq <- summary(reg.pc)$r.squared              # R^2
adrsq <- summary(reg.pc)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(reg.pc)
par(mfrow=c(1,1))
```

```{r}
reg.pc1 <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores[,1:15]))
#summary(reg.pc)

se  <- summary(reg.pc1)$sigma                  # se
rsq <- summary(reg.pc1)$r.squared              # R^2
adrsq <- summary(reg.pc1)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(reg.pc1)
par(mfrow=c(1,1))
```

```{r}
reg.pc2 <- lm(log(standf$ViolentCrimesPerPop+1)~., data=as.data.frame(pc$scores[,1:10]))
#summary(reg.pc2)

se  <- summary(reg.pc2)$sigma                  # se
rsq <- summary(reg.pc2)$r.squared              # R^2
adrsq <- summary(reg.pc2)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))

```
```{r}
par(mfrow=c(2,2))
plot(reg.pc2)
par(mfrow=c(1,1))
```


## Backward / Forward elimination

MSE, adj R2...check also the p val, confounding..
vedi slides 454 - 484 rovers

## Cross Valitation

## Lasso / Ridge Regularized models


# Other possible predictive Models
No Bayesian regression, non abbiamo possibili valori ipotesi iniziali ...
No polinomial regr.. la cond di linearità è soddisfatta....

## Add categorical predictor

state seems not influential

```{r}
is.factor(standf$state) 
#contrasts(standf$state)
corrdf["state"]=standf$state
is.factor(corrdf$state) 

regc.out <- lm(log(ViolentCrimesPerPop + 1)~. , data=corrdf) #application of LR with a categorical variable
summary(regc.out)

se  <- summary(regc.out)$sigma                  # se
rsq <- summary(regc.out)$r.squared              # R^2
adrsq <- summary(regc.out)$adj.r.squared        # adj R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2), "\n")
cat("adjusted R^2:", round(adrsq,2))
```
```{r}
par(mfrow=c(2,2))
plot(regc.out)
par(mfrow=c(1,1))
```


## Multivariate Linear Regression

questo potrebbe essere interessante per predirre anche le altre colonne crimes.. come si valuta?
per alcune response i risultati sono molto validi, magari mostriamo MSE..
```{r}
#predict all the crime features... 

mm <- lm(data.matrix(standf[,103:120]) ~., data=as.data.frame(pc$scores[,1:15]))
#summary(mm) #long output

```



## Binary classification models 
pensando di trasformare le ultime colonne in qualcosa del tipo 'HIGH'/'LOW'... ha senso? 


```{r}
# thresholf is the mean (0 since standardized)

sum(df$ViolentCrimesPerPop>0)
ViolentCrimesFactor <- rep("Low", 1996)
ViolentCrimesFactor[df$ViolentCrimesPerPop>0] <- "High"

df["ViolentCrimesPerPop"]<-as.factor(ViolentCrimesFactor)

is.factor(df$ViolentCrimesPerPop)

corrdf["ViolentCrimesPerPop"]<-as.factor(ViolentCrimesFactor)

```

### Logistic regression

```{r}
lreg.out<-glm(df$ViolentCrimesPerPop~., family = binomial, data=as.data.frame(pc$scores))

summary(lreg.out)


# check the coding of ViolentCrimesFactor
contrasts(df$ViolentCrimesPerPop)

logistic.prob <- predict(lreg.out, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(df$ViolentCrimesPerPop, logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.61] <- "Low"

table(logistic.pred, df$ViolentCrimesPerPop)

#train error
(125+174)/1996

```

```{r}
lreg.out<-glm(df$ViolentCrimesPerPop~., family = binomial, data=as.data.frame(pc$scores[,-c(11,12,14,18,19,23,26,27,seq(30,45))]))

summary(lreg.out)


# check the coding of ViolentCrimesFactor
contrasts(df$ViolentCrimesPerPop)

logistic.prob <- predict(lreg.out, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(df$ViolentCrimesPerPop, logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.67] <- "Low"

table(logistic.pred, df$ViolentCrimesPerPop)

#train error
(113+224)/1996

```

```{r}
lreg.out1<-glm(ViolentCrimesPerPop ~ ., family = binomial, data=corrdf)
summary(lreg.out)

logistic.prob <- predict(lreg.out1, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(corrdf$ViolentCrimesPerPop , logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.71] <- "Low"

table(logistic.pred, corrdf$ViolentCrimesPerPop)

#train error
(224+86)/1996

#train acc
mean(logistic.pred==corrdf$ViolentCrimesPerPop)

```

risultati strani splittando

LDA QDA non li usiamo

### KNN
```{r}
#train and test
train.X <- data.matrix(corrdf[train,-46])
test.X <- data.matrix(corrdf[-train,-46])

# train error
knn.pred <- knn(train.X, train.X, corrdf$ViolentCrimesPerPop[train], k=5)
table(knn.pred,corrdf$ViolentCrimesPerPop[train])
mean(knn.pred==corrdf$ViolentCrimesPerPop[train])

# test error
knn.pred <- knn(train.X, test.X, corrdf$ViolentCrimesPerPop[train], k=5)
table(knn.pred,corrdf$ViolentCrimesPerPop[-train])
mean(knn.pred==corrdf$ViolentCrimesPerPop[-train])
```



# Conclusions - Comparing results
Siamo stati bravi Rover, guarda che bei grafici.. (oh no)

#References

- An Introduction to Statistical Learning
- https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized
- https://online.stat.psu.edu/stat462/node/171/
- http://www.disastercenter.com/crime/uscrime.htm





