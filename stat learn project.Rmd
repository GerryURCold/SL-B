---
title: "Statistical Learning Project"
author: "Caria Natascia, Cozzolino Claudia and Petrella Alfredo"
date: "26 maggio 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Aim of the project

# Dataset Description

# Exploratory Data Analysis

We know from documentation that missing values are stored as "?".

```{r}
# import dataset
crimedata <- read.csv("crimedata.csv", na.strings="?")
View(crimedata)
dim(crimedata)
```
2215 rows and 147 columns

Attributes information:

* 4 non-predictive (communityname, state, countyCode, communityCode, fold)
* 125 predictive
* 18 potential predictor (murders, murdPerPop, rapes, rapesPerPop, robberies, robbbPerPop, assaults, assaultPerPop, burglaries,
  burglPerPop, larcenies, larcPerPop, autoTheft, autoTheftPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop)

From UCI description we know that the variables communityname and state are nominal while the remaining are all numeric.
```{r}
# check if variables communityname and state are stored as factors
is.factor(crimedata$communityname)
is.factor(crimedata$state)
```

```{r}
# check number of numeric variables: 145 (= 147 - 2) expected
sum(sapply(crimedata, is.numeric))
```

## Missing values

```{r}
# handle missing values

# inspect total NA
sum(is.na(crimedata))
mean(is.na(crimedata))
```
More than 10% of values is missing.

```{r}
# install.packages("varhandle", dependencies = T)
library(varhandle)
nadf <- inspect.na(crimedata, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=TRUE, na.value = NA)

# nadf dataframe with columns that present NA 
nadf
```
There are 41 columns with missing values, many of them with more than 50% of the data missing.
Among them:
* 3 non-predictive (countyCode, communityCode, fold)
* Many "Lemas" ---> Check info on these variables to understand what they refer
* Policing information
* Potential Predictor (rapes, rapesPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop in particular)

From documentaion:

The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the 
United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of
rapes. These resulted in missing values for rape, which resulted in missing values for per capita violent crime. Many of these omitted
communities were from the midwestern USA (Minnesota, Illinois, and Michigan have many of these).

The per capita nonviolent crime variable was calculated using the sum of crime variables considered non-violent crimes in the United
States: burglaries, larcenies, auto thefts and arsons. (There are many other types of crimes, these only include FBI 'Index Crimes').

Looking to the dataset, other suspecious values equal to 0.00 have been found
```{r}
zero_count <- function(x) sum(x==0, na.rm = TRUE)
zerodf <- lapply(crimedata, zero_count)[lapply(crimedata, zero_count)!=0]
names(zerodf)
```
Suspecious zeros: numbUrban, pctUrban.

The columns with more than 50% NA are dropped and also variables fold (no useful), OwnOccQrange, RentQrange (obtained as the difference
of other columns). We decide to keep the other columns with zero values.
```{r}
# Find columns with > 50% NA
coltodrop <- as.vector(nadf$column_name[nadf$ratio_of_NA>0.50])
coltodrop <- c(coltodrop, "fold", "OwnOccQrange", "RentQrange")
coltodrop
```

### Possible ways to handle missing values:
Firstly, all columns with > 50% NA are removed.
```{r}
# Drop columns with > 50% NA
cleandf <- crimedata[,!(names(crimedata) %in% coltodrop)]
View(cleandf)
```

Then, the remaining variables with missing values are the ones related to the crimes and another one, the variable OtherPerCap (per
capita income for people with 'other' heritage), which has only one missing value.
```{r}
# Remaining columns with NA
nadf <- inspect.na(cleandf, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=FALSE, na.value = NA)
nacolnames <- as.vector(nadf$column_name)
nacolnames
```

Possible ways to handle the remaining missing values:
1. drop all raws with at least one missing value;
2. substitute a missing value with the average computed over the state a community belongs;
3. leave NA value and consider it as another category.

We decide to proceed as in point 2.
```{r}
# dataframe of the columns which still contain NA with the mean computed over the state
meandf <- aggregate(cleandf[,nacolnames], list(cleandf$state), function(x) mean(x, na.rm = TRUE))
sum(is.na(meandf))
```
The dataframe with the mean contains NaN values, this means that there are States for which the value of a certain feature is zero for
all communities that belong to them.
This happens for IL, MI (no data of rapes, rapesPerPop, ViolentCrimesPerPop), KS, VT (no data of arsons, arsonsPerPop, nonViolPerPop).
Moreover from documentation we know that also MN has a lot of missing values for rapes (59 out of 66):
```{r}
mnNA = sum(is.na(cleandf[cleandf$state=="MN","rapes"]))
mnTot = length(cleandf[cleandf$state=="MN","rapes"])
c(mnNA, mnTot, mnNA/mnTot)
```

Therefore, data related to those states is removed.
```{r}
rowtodrop <- as.numeric(rownames(cleandf[cleandf$state %in% c("IL","MI","MN","KS","VT"),]))

cleandf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]
```

For the remaining variables with missing values, we substitute NA with the mean computed over the state
```{r}
# Substitute a missing value with the average computed over the state
for(col in nacolnames) {
  match_table <- tapply(cleandf[[col]], cleandf$state, mean, na.rm=TRUE)
  NA_position <- which(is.na(cleandf[[col]]))
  cleandf[[col]][NA_position] <- match_table[cleandf$state[NA_position]]
}
```

Now cleandf doeas not contain NA
```{r}
sum(is.na(cleandf))
```

```{r}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf", "crimedata")))
```

### Correlation insight


```{r}
# restrict to numeric independent attributes
corrdf <- cleandf[1:102]
corrdf <- corrdf[sapply(corrdf,is.numeric)]

# correlation matrix
cm <- cor(corrdf, use = 'complete.obs')
# only strongly correlated attributes highlighted
cma <- abs(cm) > 0.9
# number of strong correlations
(sum(cma)-dim(corrdf)[2])/2

# correlation matrix and filter plot
# install.packages('corrplot')
library(corrplot)
corrplot(cm,title='Correlation matrix',method='shade',type='lower',tl.pos='l',tl.cex=0.35) # order=!!!
corrplot(cma,title='Correlation matrix',method='shade',type='lower',tl.pos='l',tl.cex=0.35)

# ??? correlated couples
corrcol <- c()
for(i in 2:dim(cm)[1]) {
  for(j in 1:(i-1)) {
    if(cma[i,j]) {
      corrcol = c(corrcol,c(i,j))
    }
  }
}
corrcolname <- matrix(lapply(corrcol,function(x) colnames(corrdf)[x]),nrow=2)
corrcolname

# ??? ignore it right now, could help in the future

rem9 = c(1,9,11,17,20,28,61,65,88,92,93,94)

corrdf_ind <- corrdf[-rem9]
# View(corrdfind)

# final correlation matrix and filter
cm <- cor(corrdf_ind, use='complete.obs')
cma <- abs(cm) > 0.9

# perCapInc vs whitePerCap

corrplot(cma,title='Correlation matrix',method='shade',type='lower',tl.pos='l',tl.cex=0.35)
```

### Standardization / Normalization

```{r}
# Standardization using scale
scaledf <- cleandf
scaledf[seq(3,dim(scaledf)[2])] <- scale(cleandf[seq(3,dim(cleandf)[2])] )
```

```{r}
# Standardization using an home made function (⌐■_■)
standardization <- function(x) {
return ((x - mean(x)) / sd(x))
}

standf <- cleandf
standf[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], standardization)
```

```{r}
# Normalization using an home made function (⌐■_■)
normalization <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

normdf <- cleandf
normdf[seq(3,dim(normdf)[2])] <- lapply(cleandf[seq(3,dim(cleandf)[2])], normalization)
```

```{r, eval=FALSE, include=FALSE}
scaleddf <- data.frame(cleandf2)
scaleddf['population'] <- scale(cleandf['population'], center = rep(1/2))
# scale fa questo (con center=F): col/sqrt(sum(col^2)/(length(col)-1))
# se vogliamo la solita conviene scriverla a mano
```

### Outliers

```{r}
# Remove row if one of its value is
# 3 times greater than the upper interquartile bound or
# 3 times lower than the lower interquartile bound

is.out.IQR <- function(x){
  Q <- quantile(x, probs=c(.25, .75))
  iqr <- IQR(x)
  up <-  Q[2]+3*iqr # Upper bound  
  low <- Q[1]-3*iqr # Lower bound
  out <- (x < low | x > up)
  return(out)
}

is.out.sd <- function(x){
  sd <- sd(x)
  up <-  mean(x)+3*sd # Upper bound  
  low <- mean(x)-3*sd # Lower bound
  out <- (x < low | x > up)
  return(out)
}

outdf <- cleandf
outdf[seq(3,dim(outdf)[2])] <- lapply(cleandf[seq(3,dim(cleandf)[2])], is.out.sd)

odf <- inspect.na(outdf, hist = FALSE, byrow=TRUE, barplot = FALSE, na.value = TRUE)
rowtodrop <- odf$row_name

newdf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]
```

```{r}
# Normalization of the dataset without drastic outliers

normalization <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}

normnewdf <- newdf
normnewdf[seq(3,dim(normnewdf)[2])] <- lapply(newdf[seq(3,dim(newdf)[2])], normalization)
```

```{r}
# Remove all variables from environment excpet ...
rm(list=setdiff(ls(), c("crimedata", "cleandf", "scaledf", "standf", "normdf", "newdf", "normnewdf")))
```

## Principal Component Analysis

```{r}
# choose the version of the df you prefer (cleandf, normnewdf, scalednewdf ???)
PCAdf <- normnewdf[,3:102]

# PCA computation
pc <- princomp(PCAdf, cor=TRUE) # cor=TRUE to obtain it from the correlation matrix (use it for the unnormalized dataframe)

str(pc)
# first k principal components
k <- 40
plot(pc, npcs=k)

barplot(pc$loadings[,1], cex.names=0.5)

infl1 <- colnames(PCAdf)[abs(pc$loadings[,1])>0.15]
infl1

#install.packages('factoextra')
#library(factoextra)
#fviz_pca_var(pc, repel=TRUE) # too messy
```


## Singular Value Decomposition

An alternative way to reduce the dimensionality of the dataset is to compute the Singular Value Decomposition on the matrix $X$ associated to the model, obtaining two unitary matrices $U$ and $V$ and a rectangular diagonal matrix of singular values $S$ such that $X = USV^T$, where the values on the diagonal of $S$ are sorted in descendent order and the first $k$ columns of $V$ select the correspondent first $k$ most informative combinantions of the attributes of the original dataset, i.e. if we call $V_k$ the matrix containing the first $k$ columns of $V$, $X_k=XV_k$ is a transformation of our dataset with just $k$ columns that contains the most part of information of $X$.

```{r}
# choose the version of the df you prefer (scaledf, normdf)
SVDdf <- normnewdf[,3:102]

# SVD on the correlation matrix
sv <- svd(SVDdf)

# plot the singolar values
barplot(sv$d)

# SVD reduced model
k <- 20
V <- sv$v
matrix(SVDdf)

SVDreddf <- data.matrix(SVDdf) %*% V[,1:k]

# check that U and V are the same in this case
sum(!C.sv$u-C.sv$v<0.000001)
```

```{r, eval=FALSE, include=FALSE}
C <- cov(SVDdf)

# plot the original singular values (our actual diagonal is squared)
barplot(sqrt(C.sv$d)*(dim(SVDdf)[1]-1))
barplot(sv$d)

sv$d
sqrt(C.sv$d)*(dim(SVDdf)[1]-1)

#???
```



## State Aggregation - data analysis

```{r}
# Violent Crimes per 100K Population - State aggregation
vc_state <- aggregate(crimedata[, "ViolentCrimesPerPop"], list(crimedata$state), mean)
names(vc_state)[names(vc_state) == "Group.1"] <- "state"
names(vc_state)[names(vc_state) == "x"] <- "ViolentCrimesMean"
```

```{r}
# Population - State aggregation
pop_state <- aggregate(crimedata[, "population"], list(crimedata$state), sum)
names(pop_state)[names(pop_state) == "Group.1"] <- "state"
names(pop_state)[names(pop_state) == "x"] <- "PopTot"
```

```{r include=FALSE}
# install.packages("usmap")
library(usmap)
library(ggplot2)
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = vc_state, values = "ViolentCrimesMean", color = "red") + 
  scale_fill_continuous(low = "white", high= "red", name = "Violent Crimes", label = scales::comma) + 
  theme(legend.position = "right")
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = pop_state, values = "PopTot", color = "red") + 
  scale_fill_continuous(low = "white", high= "red", name = "Population", label = scales::comma) + 
  theme(legend.position = "right")
```
Montana, Nebraska and Hawaii are missing

## Pairs plot

```{r}
# pairs plot prova
pairs(~ medIncome+racepctblack+racePctAsian+racePctWhite+ViolentCrimesPerPop,data=cleandf)
```

```{r}
scatter.smooth(cleandf$PopDens,cleandf$ViolentCrimesPerPop)
``` 

## eventually useful stuff

* raceP(p)ct(...) columns sum up to more than 1, one hot encoding???
* numbUrban or pctUrban keep only one (maybe the second, so that we can eventually discard population)
  same for  NumUnderPov & PctPopUnderPov
            NumKidsBornNeverMar & PctKidsBornNeverMar (total missing)
            HousVacant & PctHousOccup & PctHousOwnOccup
            OwnOccLowQuart & OwnOccMedVal & OwnOccHiQuart & OwnOccQrange
            RentQrange (difference between 2 prev cols)
            Lemas(...) ALREADY REMOVED in cleandf
            PopDens (again, delete pop or similar)
            
            ?Target? & ?Target?PerPoP
* medIncome and medfamincome so similar
* PctImmigRec(...): compress in fewer columns with the coefficients describing the evolution ??? (obviously increasing (inclusion): at what rate?)
  same for PctBornSameState & PctSame(...)
* PctRec(...): same but over the full population
