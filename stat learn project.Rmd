---
title: "Statistical Learning Project"
author: "Caria Natascia, Cozzolino Claudia and Petrella Alfredo"
date: "26 maggio 2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

```{r include=FALSE}
library(car)
#install.packages("corrplot")
library(ggplot2)
#install.packages("usmap")
library(usmap)
# install.packages("varhandle", dependencies = T)
library(varhandle)

library(pROC)

library(MASS)

library(class)


set.seed(1)
```

# Aim of the project

# Dataset Description

# Data Cleaning

We know from documentation that missing values are stored as "?".

```{r}
# import dataset
crimedata <- read.csv("crimedata.csv", na.strings="?")
#View(crimedata)
dim(crimedata)
```
2215 rows and 147 columns

Attributes information:

* 4 non-predictive (communityname, state, countyCode, communityCode, fold)
* 125 predictive
* 18 potential predictor (??? intendete response?) (murders, murdPerPop, rapes, rapesPerPop, robberies, robbbPerPop, assaults, assaultPerPop, burglaries,
  burglPerPop, larcenies, larcPerPop, autoTheft, autoTheftPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop)

From UCI description (see https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized) we know that the variables communityname and state are nominal while the remaining are all numeric.
```{r}
# check if variables communityname and state are stored as factors
is.factor(crimedata$communityname)
is.factor(crimedata$state)
```

```{r}
# check number of numeric variables: 145 (= 147 - 2) expected
sum(sapply(crimedata, is.numeric))
```

## Missing values

```{r}
# handle missing values

# inspect total NA
sum(is.na(crimedata))
mean(is.na(crimedata))
```
More than 10% of values is missing.

```{r}
# install.packages("varhandle", dependencies = T)
# library(varhandle)
nadf <- inspect.na(crimedata, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=TRUE, na.value = NA)

# nadf dataframe with columns that present NA 
#nadf
```
There are 41 columns with missing values, many of them with more than 50% of the data missing.
Among them:

* 3 non-predictive (countyCode, communityCode, fold)
* Many "Lemas" ---> Check info on these variables to understand what they refer
* Policing information
* Potential Predictor (??? intendete response?) (rapes, rapesPerPop, arsons, arsonsPerPop, ViolentCrimesPerPop, nonViolPerPop in particular)

From documentaion:

The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the 
United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of
rapes. These resulted in missing values for rape, which resulted in missing values for per capita violent crime. Many of these omitted
communities were from the midwestern USA (Minnesota, Illinois, and Michigan have many of these).

The per capita nonviolent crime variable was calculated using the sum of crime variables considered non-violent crimes in the United
States: burglaries, larcenies, auto thefts and arsons. (There are many other types of crimes, these only include FBI 'Index Crimes').

Looking to the dataset, other suspecious values equal to 0.00 have been found
```{r}
zero_count <- function(x) sum(x==0, na.rm = TRUE)
zerodf <- lapply(crimedata, zero_count)[lapply(crimedata, zero_count)!=0]
names(zerodf)
```
Suspecious zeros: numbUrban, pctUrban.

The columns with more than 50% NA are dropped and so were the variables fold (not useful), OwnOccQrange and RentQrange (obtained as the difference
of other columns). We decided to keep the other columns with zero values due to their meaningfulness.
```{r}
# Find columns with > 50% NA
coltodrop <- as.vector(nadf$column_name[nadf$ratio_of_NA>0.50])

#add other columns to remove
coltodrop <- c(coltodrop, "fold", "OwnOccQrange", "RentQrange")
coltodrop
```

### Possible ways to handle missing values:
Firstly, all columns with > 50% NA are removed.
```{r}
# Drop columns with > 50% NA and clearly redundant ones
cleandf <- crimedata[,!(names(crimedata) %in% coltodrop)]
#View(cleandf)
```

Then, the remaining variables with missing values are the ones related to the crimes and another one, the variable OtherPerCap (per
capita income for people with 'other' heritage), which has only one missing value.
```{r}
# Remaining columns with NA
nadf <- inspect.na(cleandf, hist=FALSE, summary=TRUE, byrow=FALSE, barplot=FALSE, na.value = NA)
nacolnames <- as.vector(nadf$column_name)
nacolnames
```

Possible ways to handle the remaining missing values:
1. drop all raws with at least one missing value;
2. substitute a missing value with the average computed over the state a community belongs;
3. leave NA value and consider it as another category.

We decide to proceed as in point 2.
```{r}
# dataframe of the columns which still contain NA with the mean computed over the state
meandf <- aggregate(cleandf[,nacolnames], list(cleandf$state), function(x) mean(x, na.rm = TRUE))
sum(is.na(meandf))
```

The dataframe with the mean contains NaN values, this means that there are States for which the value of a certain feature is zero for
all communities that belong to them.
This happens for IL, MI (no data of rapes, rapesPerPop, ViolentCrimesPerPop), KS, VT (no data of arsons, arsonsPerPop, nonViolPerPop).
Moreover from documentation we know that also MN has a lot of missing values for rapes (59 out of 66):
```{r}
mnNA = sum(is.na(cleandf[cleandf$state=="MN","rapes"]))
mnTot = length(cleandf[cleandf$state=="MN","rapes"])
c(mnNA, mnTot, mnNA/mnTot)
```

Therefore, data related to those states is removed.
```{r}
rowtodrop <- as.numeric(rownames(cleandf[cleandf$state %in% c("IL","MI","MN","KS","VT"),]))

cleandf <- cleandf[!(rownames(cleandf) %in% rowtodrop),]
```

For the remaining variables with missing values, we substitute NA with the mean computed over the state
```{r}
# Substitute a missing value with the average computed over the state
for(col in nacolnames) {
  match_table <- tapply(cleandf[[col]], cleandf$state, mean, na.rm=TRUE)
  NA_position <- which(is.na(cleandf[[col]]))
  cleandf[[col]][NA_position] <- match_table[cleandf$state[NA_position]]
}
```

Now cleandf doeas not contain NA
```{r}
sum(is.na(cleandf))
dim(cleandf)
```

```{r}
# Remove all variables from environment excpet the cleaned and the original dataset
rm(list=setdiff(ls(), c("cleandf", "crimedata")))
```


# Exploratory Data Analysis - Assessing Assumptions for Linear Models
```{r, include=FALSE}
Col = c("#EF476F", "#FFD166", "#06D6A0", "#118AB2", "#073B4C")
Rgb = c(rgb(0.94, 0.28, 0.44, 0.7),
        rgb(1, 0.82, 0.4, 0.7),
        rgb(0.02, 0.84, 0.63, 0.7),
        rgb(0.07, 0.54, 0.7, 0.7),
        rgb(0.03, 0.23, 0.3, 0.7))
```

```{r}
# Violent Crimes per 100K Population - State aggregation - Mean
vc_state <- aggregate(cleandf[, "ViolentCrimesPerPop"], list(cleandf$state), mean)
names(vc_state)[names(vc_state) == "Group.1"] <- "state"
names(vc_state)[names(vc_state) == "x"] <- "ViolentCrimesMean"
# aggiunte
vc_state['logViolentCrimesMean'] <- log(vc_state['ViolentCrimesMean']+1)
vc_state['sqrtViolentCrimesMean'] <- sqrt(vc_state['ViolentCrimesMean'])
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = vc_state, values = "ViolentCrimesMean", color = "black") +
  scale_fill_continuous(low = Rgb[3], high = Rgb[1], name = "Violent Crimes",label=scales::comma) +
  theme(legend.position = "right")

# prova con il log
plot_usmap(data = vc_state, values = "logViolentCrimesMean", color = "black") +
  scale_fill_continuous(low = Rgb[3], high = Rgb[1], name = "log(Violent Crimes +1)",label=scales::comma) +
  theme(legend.position = "right")

# con log fa schifo, con la radice è già meglio...
plot_usmap(data = vc_state, values = "sqrtViolentCrimesMean", color = "black") +
  scale_fill_continuous(low = Rgb[3], high = Rgb[1], name = "sqrt(Violent Crimes)",label=scales::comma) +
  theme(legend.position = "right")
```

```{r}
summary(vc_state$ViolentCrimesMean)
```

```{r}
par(mfrow=c(1,2))
hist(cleandf$ViolentCrimesPerPop, prob=TRUE, breaks=30, col=Col[1], main="Violent Crimes")
lines(density(cleandf$ViolentCrimesPerPop))

hist(log(cleandf$ViolentCrimesPerPop +1), prob=TRUE, breaks=30, col=Col[1], main="log(Violent Crimes +1)")
lines(density(log(cleandf$ViolentCrimesPerPop +1)))
par(mfrow=c(1,1))

# aggiunto, da tenere? Solo se teniamo il grafico secondo me
hist(sqrt(cleandf$ViolentCrimesPerPop), prob=TRUE, breaks=30, col=Col[1], main="sqrt(Violent Crimes)")
lines(density(sqrt(cleandf$ViolentCrimesPerPop)))
par(mfrow=c(1,1))
```

```{r}
par(mfrow=c(1,2))
qqnorm(cleandf$ViolentCrimesPerPop, main="Violent Crimes")
qqline(cleandf$ViolentCrimesPerPop)

qqnorm(log(cleandf$ViolentCrimesPerPop +1), main="log(Violent Crimes +1)")
qqline(log(cleandf$ViolentCrimesPerPop +1))
par(mfrow=c(1,1))

# aggiunto, da tenere? Questo direi di no
qqnorm(sqrt(cleandf$ViolentCrimesPerPop), main="log(Violent Crimes +1)")
qqline(sqrt(cleandf$ViolentCrimesPerPop))
par(mfrow=c(1,1))
```

```{r}
violent_names <- c("murders", "rapes", "robberies", "assaults")
violPerPop_names <- c("murdPerPop", "rapesPerPop", "robbbPerPop", "assaultPerPop")

violent <- cleandf[, (colnames(cleandf) %in% violent_names)]
violPerPop <- cleandf[, (colnames(cleandf) %in% violPerPop_names)]
```

```{r}
par(mfrow = c(2,2))
boxplot(violent, outline = TRUE, col = Col[seq(1:4)])
boxplot(violent, outline = FALSE, col = Col[seq(1:4)])

boxplot(violPerPop, outline = TRUE, col = Col[seq(1:4)])
boxplot(violPerPop, outline = FALSE, col = Col[seq(1:4)])
par(mfrow=c(1,1))
```

```{r}
# Non Violent Crimes per 100K Population - State aggregation - Mean
nvc_state <- aggregate(cleandf[, "nonViolPerPop"], list(cleandf$state), mean)
names(nvc_state)[names(nvc_state) == "Group.1"] <- "state"
names(nvc_state)[names(nvc_state) == "x"] <- "NonViolentCrimesMean"
```

```{r, fig.height = 3, fig.align = "center"}
par(mfrow=c(1,1))
plot_usmap(data = nvc_state, values = "NonViolentCrimesMean", color = "black") +
  scale_fill_continuous(low = Rgb[3], high = Rgb[1], name = "Non Violent Crimes",label=scales::comma) +
  theme(legend.position = "right")
```

```{r}
par(mfrow=c(1,2))
hist(cleandf$nonViolPerPop, prob=TRUE, breaks=30, col=Col[2], main="Non Violent Crimes")
lines(density(cleandf$nonViolPerPop))

hist(log(cleandf$nonViolPerPop+1), prob=TRUE, breaks=30, col=Col[2], main="log(Non Violent Crimes +1)")
lines(density(log(cleandf$nonViolPerPop+1)))
par(mfrow=c(1,1))
```

```{r}
par(mfrow=c(1,2))
qqnorm(cleandf$nonViolPerPop, main="Non Violent Crimes")
qqline(cleandf$nonViolPerPop)

qqnorm(log(cleandf$nonViolPerPop +1), main="log(Non Violent Crimes +1)")
qqline(log(cleandf$nonViolPerPop +1))
par(mfrow=c(1,1))
```

```{r}
nonViol_names <- c("burglaries", "larcenies", "autoTheft", "arsons")
nonViolPerPop_names <- c("burglPerPop", "larcPerPop", "autoTheftPerPop", "arsonsPerPop")

nonViol <- cleandf[, (colnames(cleandf) %in% nonViol_names)]
nonViolPerPop <- cleandf[, (colnames(cleandf) %in% nonViolPerPop_names)]
```

```{r}
par(mfrow = c(2,2))
boxplot(nonViol, outline = TRUE, col = Col[seq(1:4)])
boxplot(nonViol, outline = FALSE, col = Col[seq(1:4)])

boxplot(nonViolPerPop, outline = TRUE, col = Col[seq(1:4)])
boxplot(nonViolPerPop, outline = FALSE, col = Col[seq(1:4)])
par(mfrow=c(1,1))
```

## Altri Plot a caso
### State Aggregation - data analysis

Maps plot on mean values per state ... vogliamo in caso tenerne qualcuno?

```{r, include=FALSE}
#plot mean some predictor and all response vars per state

dataf=cleandf


for(i in c(3,13,26,29,44, seq(dim(dataf)[2]-16,dim(dataf)[2]-2, by=2),dim(dataf)[2]-1,dim(dataf)[2])) {
  dataaggr <- aggregate(dataf[, colnames(dataf)[i]], list(dataf$state), mean)
  names(dataaggr)[names(dataaggr) == "Group.1"] <- "state"
  names(dataaggr)[names(dataaggr) == "x"] <- colnames(dataf)[i]
  
  mycol=c("#00AFBB", "#E7B800", "#FC4E07")[i%%3 + 1]


  pl<-plot_usmap(data = dataaggr, values = colnames(dataf)[i], color = 1) + 
    scale_fill_continuous(low = "white", high= mycol, name = colnames(dataf)[i], label = scales::comma) + 
    theme(legend.position = "right")
  
  print(pl)

}
```

### Pairs plot


```{r}
# pairs plot prova
pairs(~ medIncome+racepctblack+racePctAsian+racePctWhite+ViolentCrimesPerPop,data=cleandf)
```

```{r}
#scatter plot between all predictor vs ViolentCrimesPerPop (tantii)
par(mfrow=c(2,2))

#dataf=cleandf
#seq(3,dim(dataf)[2]-18)

#for(i in seq(3,dim(dataf)[2]-18)) {
  
  #scatter.smooth(dataf[,i],dataf$ViolentCrimesPerPop, xlab=colnames(dataf[i]), ylab='ViolentCrimesPerPop')

#}

par(mfrow=c(1,1))

```




# Split in Train and Test / validation Folds
da mettere prima dei primi modelli
```{r}
train <- sample(rownames(cleandf), 1600)
```

# Multiple linear regression: response = ViolentCrimesPerPop

Remove non predictive variables:
communityname, state, all crimes except variable ViolentCrimesPerPop
```{r}
coltodrop <- c(1,2, seq(103,120)[-17]) # -17 keeps ViolentCrimesPerPop

df <- cleandf[,-coltodrop]
```

Multiple Linear Regression
```{r}
reg.out <- lm(df$ViolentCrimesPerPop~., data=df)
```

```{r}
se  <- summary(reg.out)$sigma         # se
rsq <- summary(reg.out)$r.squared     # R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2))
```

```{r}
par(mfrow=c(2,2))
plot(reg.out)
par(mfrow=c(1,1))
```

```{r}
# train and test MSE

# linear model with training data
lm.fit <- lm(df$ViolentCrimesPerPop~., data=df, subset=train)


# mean squared error on train
y.pred <- predict(lm.fit, newdata=df[train, ])
MSE <- mean((df$ViolentCrimesPerPop[train]-y.pred)^2)
MSE

# mean squared error on test
y.pred <- predict(lm.fit, newdata=df[-train, ])
MSE <- mean((df$ViolentCrimesPerPop[-train]-y.pred)^2)
MSE 
```


## Assessing Model Assumptions:
The model assumptions are:

- Linearity of the response-predictor relationships;
- Homeschedasticity: Var($\epsilon_i$) = $\sigma^2$;
- Normality and Independence of $\epsilon_i$.

### Linearity
The plots of residuals versus fitted values shows a little pattern, however it seems to indicate that there are linear associations in
the data. (e ci dice che gli errori sono non correlati :))

### Homoschedasticity
The presence of a funnel shape in the residual plot suggests that the error terms do not have a constant variance. One possible solution
is to transform the response variable $Y$ using a concave function such as $\log(Y)$ or $\sqrt(Y)$ .

```{r}
reg.out2 <- lm(log(df$ViolentCrimesPerPop+1)~., data=df)
```

```{r}
se  <- summary(reg.out2)$sigma         # se
rsq <- summary(reg.out2)$r.squared     # R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2))
```

```{r}
par(mfrow=c(2,2)) 
plot(reg.out2)
par(mfrow=c(1,1))
```

The plots of residuals versus fitted values shows that such a transformation leads to a reduction in heteroscedasticity.

```{r}
#train and test MSE

# linear model with training data
lm.fit2 <- lm(log(df$ViolentCrimesPerPop+1)~., data=df, subset=train)


# mean squared error on train
y.pred <- predict(lm.fit2, newdata=df[train, ])
MSE <- mean((log(df$ViolentCrimesPerPop[train]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit2, newdata=df[-train, ])
MSE <- mean((log(df$ViolentCrimesPerPop[-train]+1)-y.pred)^2)
MSE 
```

### Normality and Independence of errors
We assess normality looking to the QQplot....


## Other Possible Problems

### Outliers
The residual plot identifies some outliers. However, it can be difficult to decide how large a residual needs to be before we consider
the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals, computed
by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute
value are possible outliers.

Note: Why 3? The Studentized Residuals are approximated by a N(0,1). The probability to observe a value greater than 3 is 0.001349898.
```{r}
1 - pnorm(3)
```

```{r}
plot(predict(reg.out2), rstandard(reg.out2), xlab="Fitted Values", ylab = "Studentized Residuals")

abline(h=3, col = "red")
abline(h=-3, col = "red")
```

```{r}
out <- as.numeric(names(rstandard(reg.out2)[(abs(rstandard(reg.out2)) > 3)]))
outdf <- df[!(rownames(df) %in% out),]
```

```{r}
reg.out3 <- lm(log(outdf$ViolentCrimesPerPop+1)~., data=outdf)
```

```{r}
se  <- summary(reg.out3)$sigma         # se
rsq <- summary(reg.out3)$r.squared     # R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2))
```

```{r}
par(mfrow=c(2,2)) 
plot(reg.out3)
par(mfrow=c(1,1))
```

```{r}
#train and test MSE
to=sample(1:1978,1590)
# linear model with training data
lm.fit3 <- lm(log(outdf$ViolentCrimesPerPop+1)~., data=outdf, subset=to)


# mean squared error on train
y.pred <- predict(lm.fit3, newdata=outdf[to, ])
MSE <- mean((log(outdf$ViolentCrimesPerPop[to]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit3, newdata=outdf[-to, ])
MSE <- mean((log(outdf$ViolentCrimesPerPop[-to]+1)-y.pred)^2)
MSE 
```

The RSE is 0.65 when the outlier is included in the regression and it is 0.59 when the outlier is removed. Since the RSE is used to
compute all confidence intervals and p-values, this can have implications for the interpretation of the fit. Moreover, dropping the
outliers the $R^2$ increses from 0.67 to 0.7.
Care should be taken in the decision of taking or dropping outliers.

### High Leverage Points
In order to quantify an observations leverage, we compute the leverage statistic. If a given observation has a leverage statistic that
greatly exceeds (p+1)/n, then we may suspect that the corresponding point has high leverage.

```{r}
plot(hatvalues(reg.out2), rstandard(reg.out2), xlab="Leverage", ylab = "Studentized Residuals")

p <- dim(df)[2]-1
n <- dim(df)[1]
abline(v=(p+1)/n, col = "red")
```
E ora....???

Try removing high leverage points vedi https://online.stat.psu.edu/stat462/node/171/

```{r}

whereh<-hatvalues(reg.out2)>3*(p+1)/n
sum(whereh) # 56 punti

#hatvalues contains the diag of the hat matrix


plot(hatvalues(reg.out2)[!whereh], rstandard(reg.out2)[!whereh], xlab="Leverage", ylab = "Studentized Residuals")
abline(v=(p+1)/n, col = "red")

reg.out4 <- lm(log(df$ViolentCrimesPerPop[!whereh]+1)~., data=df[!whereh,])

plot(reg.out4)

se  <- summary(reg.out4)$sigma         # se
rsq <- summary(reg.out4)$r.squared     # R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2))


#computing H by hand

#X matrix of predictors values
#X<- data.matrix(df[,1:100])
#H <- X %*% solve(t(X) %*% X) %*% t(X)

#diag(H) #uguale ad hatvalues yee

```

Con questo cut non sembra esserci molta differenza... ma RSE diminuisce di qualche millesimo.. e togliamo solo 56 righe... leggo dalla source sopra la seguente...:
a high leverage observation may or may not actually be influential, a data point has large influence only if it affects the estimated regression function.. potrebbe essere una buona notizia :)

Facendo un cut maggiore usando 2*.. l'errore riaumenta

```{r}
#train and test MSE

# linear model with training data
lm.fit4 <- lm(log(df$ViolentCrimesPerPop[!whereh]+1)~., data=df[!whereh,], subset=train[-whereh])


# mean squared error on train
y.pred <- predict(lm.fit4, newdata=df[train[-whereh], ])
MSE <- mean((log(df$ViolentCrimesPerPop[train[-whereh]]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit4, newdata=df[-c(train,whereh), ])
MSE <- mean((log(df$ViolentCrimesPerPop[-c(train,whereh)]+1)-y.pred)^2)
MSE 
```


### Collinearity 
Collinearity refers to the situation in which two or more predictor variables
collinearity
are closely related to one another. The presence of collinearity can pose problems in
the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response.

Since collinearity reduces the accuracy of the estimates of the regression
coefficients, it causes the standard error for $\hat \beta_j$ to grow. Recall that the
t-statistic for each predictor is calculated by dividing $\hat \beta_j$ by its standard
error. Consequently, collinearity results in a decline in the t-statistic. As a
result, in the presence of collinearity, we may fail to reject $H_0:\ \beta_j = 0$. This
means that the power of the hypothesis test-the probability of correctly
power
detecting a non-zero coefficient-is reduced by collinearity. 

To avoid such a situation, it is desirable to identify and address potential
collinearity problems while fitting the model.
A simple way to detect collinearity is to look at the correlation matrix
of the predictors.

####Correlation insight

In the following few lines of code we perform a first manual skimming of strongly dependent variables removing, between the most
correlated ones, the more redundant and meaningless.

```{r}
# restrict to the predictive numeric attributes
#corrdf <- cleandf[1:102]
#corrdf <- corrdf[sapply(corrdf,is.numeric)] # first two columns removed
corrdf <- df[1:100]
#corrdf e come df senza ultima colonna, magari mettiamo quella cosi' non abbiamo 1000 dframes

#View(corrdf)

# correlation matrix
cm <- cor(corrdf, use='complete.obs')

# install.packages('corrplot')
library(corrplot)
# correlation matrix plot
corrplot(cm,title='Initial correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35) # order=!!!

# correlation tradeoff
threshold <- 0.7
# only strongly correlated attributes highlighted
cma <- abs(cm) > threshold
# number of strong correlations
(sum(cma) - dim(corrdf)[2]) / 2

# filtered correlation matrix plot
corrplot(cma,title='Filtered correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)
```

??? Spiega le scelte e soprattutto quella a 0.7!

```{r}
# columns with correlation with more meaningful ones higher than threshold in absolute value
rem9 <- c(1,9,11,17,20,21,28,30,38,39,41,44,45,50,54,56,58,59,60,61,64,65,74,80,82,83,85,86,90,91,92)
rem8 <- c(rem9,2,4,8,13,14,18,29,43,46,49,51,55,62,69,81,84,95)
rem7 <- c(rem8,16,31,32,63,70,71,96)
#colnames(corrdf)[rem7]

corrdf_ind <- corrdf[-rem7]

# final correlation matrix and filter
cm_ind <- cor(corrdf_ind, use='complete.obs')
cma_ind <- abs(cm_ind) > threshold

# resulting filtered correlation matrix plot
corrplot(cma_ind, title='Filtered correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)

# 0.9
# OK perCapInc del. vs whitePerCap (21 del. vs 22, use average)
# OK PctBSorMore kept vs PctOccupMgmtProf (32 kept vs 38, solved by 0.8)
# OK NumKidsBornNeverMar del. vs NumImmig (50 del. vs 52, solved by 0.8)
# 0.8
# OK racepvtblack vs racePctWhite (3 kept vs 4, let's keep minorities)
# OK agePct12t21 kept vs agePct12t29 (7 kept vs 8, solved by 0.7)
# OK medIncome del. vs whitePerCap (13 del. vs 22, use average)
# OK pctWPubAsst del. vs PctUnemployed (18 del. vs 33, solved by 0.7)
# OK PctPopUnderPov del. vs PctHousNoPhone (29 del. vs 78, solved by 0.7)
# OK PctYoungKids2Par del. vs PctTeen2Par (46 del. vs 47, solved by 0.7)
# OK PctWorkMomYoungKids kept vs PctWorkMom (48 kept vs 49, unanimity)
# OK PctImmigRecent kept vs PctImmigRec8 (53 kept vs 55)
# OK PctSameHouse85 kept vs PctSameCity85 (94 kept vs 95, more informative)
# 0.7
# OK NumImmig and HousVacant (52 and 72, unanimity)

coltodropcorr <- colnames(corrdf)[rem7]

cleandfcorr <- cleandf[,!(names(cleandf) %in% coltodropcorr)]

# final correlation matrix plot
corrplot(cm_ind, title='Final correlation matrix', method='color', type='lower', tl.pos='l', tl.col='black', tl.cex=0.35)
```


#### Multicollinearity
Unfortunately, not all collinearity problems can be
detected by inspection of the correlation matrix.... VIF? 

```{r}

reg.out2 <- lm(log(df$ViolentCrimesPerPop+1)~., data=df)

sum(vif(reg.out2)>10) #siamo nella cacca? 68 colonne da qui... ignoriamo la cosa e andiamo avanti? piu che atro non ho capito bene come funziona questo vif

```

When faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression
fit, since the presence of collinearity implies that the information that this
variable provides about the response is redundant in the presence of the
other variables. The second solution is
to combine the collinear variables together into a single predictor.



qui da provare il modello con le colonne tolte dalle analisi sulle collinearity??
```{r}

reg.out5 <- lm(log(cleandfcorr$ViolentCrimesPerPop+1)~., data=cleandfcorr[-c(1,2, seq(48,65)[-17])
])

plot(reg.out5)

se  <- summary(reg.out5)$sigma         # se
rsq <- summary(reg.out5)$r.squared     # R^2
cat("RSE:", round(se,2), "\n")
cat("R^2:", round(rsq,2))

#summary(reg.out5)

```

RSE aumenta.. ma massimo R^2 fino ad ora

```{r}
#train and test MSE

# linear model with training data
lm.fit5 <- lm(log(cleandfcorr$ViolentCrimesPerPop+1)~., data=cleandfcorr[-c(1,2, seq(48,65)[-17])], subset=train)


# mean squared error on train
y.pred <- predict(lm.fit5, newdata=cleandfcorr[train,-c(1,2, seq(48,65))])
MSE <- mean((log(cleandfcorr$ViolentCrimesPerPop[train]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fit5, newdata=cleandfcorr[-train,-c(1,2, seq(48,65))])
MSE <- mean((log(cleandfcorr$ViolentCrimesPerPop[-train]+1)-y.pred)^2)
MSE 
```

# Other possible transforms of data
## Standardization

```{r}
# Standardization using scale
scaledf <- cleandf
scaledf[seq(3,dim(scaledf)[2])] <- scale(cleandf[seq(3,dim(cleandf)[2])] )
```

## Outliers
```{r}
# Remove row if one of its value is
# 3 times greater than the upper interquartile bound or
# 3 times lower than the lower interquartile bound

is.out.IQR <- function(x){
  Q <- quantile(x, probs=c(.25, .75))
  iqr <- IQR(x)
  up <-  Q[2]+3*iqr # Upper bound  
  low <- Q[1]-3*iqr # Lower bound
  out <- (x < low | x > up)
  return(out)
}

is.out.sd <- function(x){
  sd <- sd(x)
  up <-  mean(x)+3*sd # Upper bound  
  low <- mean(x)-3*sd # Lower bound
  out <- (x < low | x > up)
  return(out)
}

temp <- standf
temp[seq(3,dim(standf)[2])] <- lapply(standf[seq(3,dim(standf)[2])], is.out.IQR)

temp <- inspect.na(temp, hist=FALSE, byrow=TRUE, barplot=FALSE, na.value=TRUE)
rowtodrop_s <- temp$row_name

standf_nO <- standf[!(rownames(standf) %in% rowtodrop_s),]
```

```{r}
# Remove all variables from environment excpet ...
rm(list=setdiff(ls(), c("crimedata", "cleandf", "scaledf", "df", "train")))
```


# Variables selection

## Principal Component Analysis

```{r}
# choose the version of the df you prefer (cleandf, scaledf)
PCAdf <- standf[,3:(dim(standf)[2]-18)]

# PCA computation
pc <- princomp(PCAdf, cor=TRUE) # cor=TRUE to obtain it from the correlation matrix (use it for the unnormalized dataframe)

str(pc)
# first k principal components
k <- 40
plot(pc, npcs=k, cex.names=0.5,las=2, col="#00AFBB", main='Principal Components')

barplot(pc$loadings[,1], cex.names=0.5,las=2, col=  "#FC4E07")

infl1 <- colnames(PCAdf)[abs(pc$loadings[,1])>0.15]
infl1

#install.packages('factoextra')
library(factoextra)
fviz_pca_var(pc,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T,     # Avoid text overlapping
             select.var = list(contrib = 15)
             )




PCAdfcorr <- standfcorr[,3:(dim(standfcorr)[2]-18)]

# PCA computation
pccorr <- princomp(PCAdfcorr, cor=TRUE) # cor=TRUE to obtain it from the correlation matrix (use it for the unnormalized dataframe)

str(pccorr)
# first k principal components
k <- 40
plot(pccorr, npcs=k, cex.names=0.5,las=2, col="#00AFBB", main='Principal Components')

barplot(pccorr$loadings[,1], cex.names=0.5,las=2, col=  "#FC4E07")

infl1corr <- colnames(PCAdfcorr)[abs(pccorr$loadings[,1])>0.15]
infl1corr

#install.packages('factoextra')
library(factoextra)
fviz_pca_var(pccorr,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = T,     # Avoid text overlapping
             select.var = list(contrib = 15)
             )
```


PC regression
```{r}

#use corresponding pc 
pc0 <- lm(standf$ViolentCrimesPerPop~., data=as.data.frame(pc$scores))
summary(pc0)

#use only first 15 comp
pc01 <- lm(standf$ViolentCrimesPerPop~., data=as.data.frame(pc$scores[,1:15]))
summary(pc01)

#use corresponding pc 
pc1 <- lm(standfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores))
summary(pc1)

#use only first 10 comp
pc11 <- lm(standfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores[,1:15]))
summary(pc11)

#use only comp with '***' significativity
pc12 <- lm(standfcorr$ViolentCrimesPerPop~., data=as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)]))
summary(pc12)

plot(pc12)

scatter.smooth(pccorr$scores[,8],standfcorr$ViolentCrimesPerPop)
```

no log transf

```{r}
#train and test MSE
tp<-sample(1:1280,1000)
pcdf<-as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])

# linear model with training data
lm.fitPC <- lm(standfcorr$ViolentCrimesPerPop~., data=pcdf, subset=tp)


# mean squared error on train
y.pred <- predict(lm.fitPC, newdata=pcdf[tp,])
MSE <- mean((standfcorr$ViolentCrimesPerPop[tp]-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fitPC, newdata=pcdf[-tp,])
MSE <- mean((standfcorr$ViolentCrimesPerPop[-tp]-y.pred)^2)
MSE 
```

con trasf
```{r}
#train and test MSE
tp<-sample(1:1280,1000)
pcdf<-as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])

# linear model with training data
lm.fitPC <- lm(log(standfcorr$ViolentCrimesPerPop+1)~., data=pcdf, subset=tp)


# mean squared error on train
y.pred <- predict(lm.fitPC, newdata=pcdf[tp,])
MSE <- mean((log(standfcorr$ViolentCrimesPerPop[tp]+1)-y.pred)^2)
MSE 

# mean squared error on test
y.pred <- predict(lm.fitPC, newdata=pcdf[-tp,])
MSE <- mean((log(standfcorr$ViolentCrimesPerPop[-tp]+1)-y.pred)^2)
MSE 
```

## Singular Value Decomposition

An alternative way to reduce the dimensionality of the dataset is to compute the Singular Value Decomposition on the matrix $X$
associated to the model, obtaining two unitary matrices $U$ and $V$ and a rectangular diagonal matrix of singular values $S$ such that
$X = USV^T$, where the values on the diagonal of $S$ are sorted in descendent order and the first $k$ columns of $V$ select the
correspondent first $k$ most informative combinantions of the attributes of the original dataset, i.e. if we call $V_k$ the matrix
containing the first $k$ columns of $V$, $X_k=XV_k$ is a transformation of our dataset with just $k$ columns that contains the most part
of information of $X$.

```{r}
# scaledf SVD
SVDdf <- scaledf[,3:102]

# SVD on the correlation matrix
sv <- svd(SVDdf)

# plot the singolar values
barplot(sv$d)

# SVD reduced model
k <- 20
V <- sv$v
matrix(SVDdf)

SVDreddf <- data.matrix(SVDdf) %*% V[,1:k]

#C.sv??
# check that U and V are the same in this case
#sum(!C.sv$u-C.sv$v<0.000001)
```

```{r, eval=FALSE, include=FALSE}
C <- cov(SVDdf)

# plot the original singular values (our actual diagonal is squared)
#barplot(sqrt(C.sv$d)*(dim(SVDdf)[1]-1))
barplot(sv$d)

sv$d
#sqrt(C.sv$d)*(dim(SVDdf)[1]-1)

#???
```

## Backward - Forward elimination

or other greedy search

aic Bic, Cp, adj R2...check also the p val, confounding..
vedi slides 454 - 484 rovers

## Cross Valitation

# Other possible predictive Models

## Polynomial Regression

non ci serve, linearity assump soddisfatta
DA TOGLIERE, GIA FATTO SU
```{r, eval=FALSE, include=FALSE}
# fit using a second order polynomial in all var

polyformula <- eval(paste("standfcorr$ViolentCrimesPerPop ~",
paste(paste('poly(', names(as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])), ', 2)', sep=''), collapse=" + ")))

m2 <- lm(polyformula, data=as.data.frame(pccorr$scores))
summary(m2)

plot(m2)



# fit using a 3 order polynomial in all var

polyformula <- eval(paste("standfcorr$ViolentCrimesPerPop ~",
paste(paste('poly(', names(as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)])), ', 3)', sep=''), collapse=" + ")))

m3 <- lm(polyformula, data=as.data.frame(pccorr$scores))
summary(m3)

plot(m3)



#usare solo se si hanno poche var in considerazione! dai risultati sembra che gli ordini superiori al primo non siano molto influenti...
```

## Multivariate Linear Regression

questo potrebbe essere interessante per predirre anche le altre colonne crimes.. come si valuta?
```{r}
#predict all the crime features... 
mm <- lm(data.matrix(standfcorr[,48:65]) ~. , data=as.data.frame(pccorr$scores[,c(1,2,5,8,9,10)]))

#summary(m3) #long output

```

## Bayesian regression?
Issue... non abbiamo possibili valori ipotesi iniziali ...

## Binary classification models 
pensando di trasformare le ultime colonne in qualcosa del tipo 'HIGH'/'LOW'... ha senso? 


http://www.disastercenter.com/crime/uscrime.htm

```{r}
avgviol<- 731

sum(df$ViolentCrimesPerPop>avgviol)
ViolentCrimesFactor <- rep("Low", 1996)
ViolentCrimesFactor[df$ViolentCrimesPerPop>avgviol] <- "High"
df["ViolentCrimesFactor"]<-as.factor(ViolentCrimesFactor)

is.factor(cleandfcorr$ViolentCrimesFactor)

cleandfcorr["ViolentCrimesFactor"]<-as.factor(ViolentCrimesFactor)

```

### Logistic regression
```{r}
lreg.out<-glm(df$ViolentCrimesFactor ~ ., family = binomial, data=df[,-101])
summary(lreg.out)


# check the coding of ViolentCrimesFactor
contrasts(df$ViolentCrimesFactor)

logistic.prob <- predict(lreg.out, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(df$ViolentCrimesFactor, logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.75] <- "Low"

table(logistic.pred, df$ViolentCrimesFactor)

#train error
(226+63)/1996

```

```{r}
lreg.out1<-glm(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47])
summary(lreg.out)

logistic.prob <- predict(lreg.out1, type="response") # want probability

# ROC curve 

# levels = controls (0's) as first element and  cases (1's) as second
roc.out <- roc(cleandfcorr$ViolentCrimesFactor, logistic.prob, levels=c("Low", "High"))

auc(roc.out)

plot(roc.out,  print.auc=TRUE, legacy.axes=TRUE, xlab="False positive rate", ylab="True positive rate")

# threshold that maximises the sum of sensitivity and specificity
coords(roc.out, "best")
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.71] <- "Low"

table(logistic.pred, cleandfcorr$ViolentCrimesFactor)

#train error
(224+86)/1996

#train acc
mean(logistic.pred==cleandfcorr$ViolentCrimesFactor)

```

```{r}
#using train and test
lreg.out1<-glm(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47], subset = train)

logistic.prob <- predict(lreg.out1, type="response") # want probability
logistic.pred <- rep("High", 1996)
logistic.pred[logistic.prob>0.71] <- "Low"
table(logistic.pred, cleandfcorr$ViolentCrimesFactor)

#train acc
mean(logistic.pred[train]==cleandfcorr$ViolentCrimesFactor[train])
#test acc
mean(logistic.pred[-train]==cleandfcorr$ViolentCrimesFactor[-train])

```
risultati strani splittando

### LDA e QDA
servono delle assuzioni maggiori sui dati.. da controllare, discuterne
```{r}
lda.fit <- lda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47])
lda.fit

lda.pred <- predict(lda.fit, cleandfcorr[,3:47])
lda.class <- lda.pred$class
table(lda.class,cleandfcorr$ViolentCrimesFactor)

#train acc
mean(lda.class==cleandfcorr$ViolentCrimesFactor)

#train err
(87+183)/1996

qda.fit <- qda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47])
qda.fit
qda.class <- predict(qda.fit,cleandfcorr[,3:47])$class
table(qda.class,cleandfcorr$ViolentCrimesFactor)
#train err
(118+189)/1996
#train acc
mean(qda.class==cleandfcorr$ViolentCrimesFactor)
```


```{r}
#using train and test
lda.fit <- lda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47], subset=train)
#lda.fit

lda.pred <- predict(lda.fit, cleandfcorr[,3:47])
lda.class <- lda.pred$class
table(lda.class,cleandfcorr$ViolentCrimesFactor)

#train acc
mean(lda.class[train]==cleandfcorr$ViolentCrimesFactor[train])
#test acc
mean(lda.class[-train]==cleandfcorr$ViolentCrimesFactor[-train])


qda.fit <- qda(cleandfcorr$ViolentCrimesFactor ~ ., family = binomial, data=cleandfcorr[,3:47],subset=train)
#qda.fit
qda.class <- predict(qda.fit,cleandfcorr[,3:47])$class
table(qda.class,cleandfcorr$ViolentCrimesFactor)

#train acc
mean(qda.class[train]==cleandfcorr$ViolentCrimesFactor[train])
#test acc
mean(qda.class[-train]==cleandfcorr$ViolentCrimesFactor[-train])

```


### KNN
```{r}

train.X <- data.matrix(cleandfcorr[,3:47])

knn.pred <- knn(train.X, train.X, cleandfcorr$ViolentCrimesFactor, k=3)
table(knn.pred,cleandfcorr$ViolentCrimesFactor)
(161+97)/1996

mean(knn.pred==cleandfcorr$ViolentCrimesFactor)
```
va meglio.. 

```{r}
#train and test
train.X <- data.matrix(cleandfcorr[train,3:47])
test.X <- data.matrix(cleandfcorr[-train,3:47])

# train error
knn.pred <- knn(train.X, train.X, cleandfcorr$ViolentCrimesFactor[train], k=5)
table(knn.pred,cleandfcorr$ViolentCrimesFactor[train])
mean(knn.pred==cleandfcorr$ViolentCrimesFactor[train])

# test error
knn.pred <- knn(train.X, test.X, cleandfcorr$ViolentCrimesFactor[train], k=5)
table(knn.pred,cleandfcorr$ViolentCrimesFactor[-train])
mean(knn.pred==cleandfcorr$ViolentCrimesFactor[-train])
```

## Regularized models


# Conclusions - Comparing results
Siamo stati bravi Rover, guarda che bei grafici.. (oh no)

#References

- An Introduction to Statistical Learning
- https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime+Unnormalized

#eventually useful stuff

* raceP(p)ct(...) columns sum up to more than 1, one hot encoding???
* numbUrban or pctUrban keep only one (maybe the second, so that we can eventually discard population)
  same for  NumUnderPov & PctPopUnderPov
            NumKidsBornNeverMar & PctKidsBornNeverMar (total missing)
            HousVacant & PctHousOccup & PctHousOwnOccup
            OwnOccLowQuart & OwnOccMedVal & OwnOccHiQuart & OwnOccQrange
            RentQrange (difference between 2 prev cols)
            Lemas(...) ALREADY REMOVED in cleandf
            PopDens (again, delete pop or similar)
            
            ?Target? & ?Target?PerPoP
* medIncome and medfamincome so similar
* PctImmigRec(...): compress in fewer columns with the coefficients describing the evolution ??? (obviously increasing (inclusion): 
at what rate?)
  same for PctBornSameState & PctSame(...)
* PctRec(...): same but over the full population

